{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bacc318390456b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook focuses on finetuning the terramind model to identify floods in a sentinel scene. The main take aways from this notebook will be as follows:\n",
    "1. Learn how to use Terratorch to finetune terramind for floods in sentinel scene.\n",
    "2. Understand the effects of spefic parameters in training and hardware utilization.\n",
    "\n",
    "**Note:** The entirety of this notebook is tuned to work well in a sagemaker environment. Sagemaker Training Job will be used to train the models. If you are interested in running this in a colab environment, please leverage [ESA NASA Foundation Model Workshop](https://github.com/NASA-IMPACT/ESA-NASA-workshop-2025/tree/main/Track%201%20(EO)/TerraMind) materials.\n",
    "\n",
    "## Setup\n",
    "1. Go to \"Kernel\"\n",
    "2. Select \"terramind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "import sagemaker\n",
    "import terratorch\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from datetime import time\n",
    "from glob import glob\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b65b8e7cd7d65",
   "metadata": {},
   "source": [
    "3. Download the dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dw5-9A4A4OmI",
   "metadata": {
    "id": "dw5-9A4A4OmI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir ../data/\n",
    "if not os.path.isfile(\"../data/sen1floods11_v1.1.tar.gz\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1lRw3X7oFNq_WyzBO6uyUJijyTuYm23VS\", '../data/sen1floods11_v1.1.tar.gz')\n",
    "\n",
    "!tar -xzf ../data/sen1floods11_v1.1.tar.gz -C ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f776454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "dataset_path = Path(\"../data/sen1floods11_v1.1\")\n",
    "BUCKET_NAME = <BUCKET_NAME>\n",
    "\n",
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "all_files = sagemaker_session.upload_data(path=f'{dataset_path}', bucket=BUCKET_NAME, key_prefix='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b7e3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed_everything': 42,\n",
       " 'trainer': {'accelerator': 'auto',\n",
       "  'strategy': 'auto',\n",
       "  'devices': 'auto',\n",
       "  'num_nodes': 1,\n",
       "  'precision': '16-mixed',\n",
       "  'logger': True,\n",
       "  'callbacks': [{'class_path': 'RichProgressBar'},\n",
       "   {'class_path': 'LearningRateMonitor',\n",
       "    'init_args': {'logging_interval': 'epoch'}},\n",
       "   {'class_path': 'ModelCheckpoint',\n",
       "    'init_args': {'dirpath': '../output/burnscars/checkpoints',\n",
       "     'mode': 'max',\n",
       "     'monitor': 'val/Multiclass_Jaccard_Index',\n",
       "     'filename': 'best-mIoU',\n",
       "     'save_weights_only': True}}],\n",
       "  'max_epochs': 100,\n",
       "  'log_every_n_steps': 5,\n",
       "  'default_root_dir': '../output/terramind_base_sen1floods11/'},\n",
       " 'data': {'class_path': 'terratorch.datamodules.GenericMultiModalDataModule',\n",
       "  'init_args': {'task': 'segmentation',\n",
       "   'batch_size': 4,\n",
       "   'num_workers': 4,\n",
       "   'modalities': ['S2L1C', 'S1GRD'],\n",
       "   'rgb_modality': 'S2L1C',\n",
       "   'rgb_indices': [3, 2, 1],\n",
       "   'train_data_root': {'S2L1C': '../data/sen1floods11_v1.1/data/S2L1CHand',\n",
       "    'S1GRD': '../data/sen1floods11_v1.1/data/S1GRDHand'},\n",
       "   'train_label_data_root': '../data/sen1floods11_v1.1/data/LabelHand',\n",
       "   'val_data_root': {'S2L1C': '../data/sen1floods11_v1.1/data/S2L1CHand',\n",
       "    'S1GRD': '../data/sen1floods11_v1.1/data/S1GRDHand'},\n",
       "   'val_label_data_root': '../data/sen1floods11_v1.1/data/LabelHand',\n",
       "   'test_data_root': {'S2L1C': '../data/sen1floods11_v1.1/data/S2L1CHand',\n",
       "    'S1GRD': '../data/sen1floods11_v1.1/data/S1GRDHand'},\n",
       "   'test_label_data_root': '../data/sen1floods11_v1.1/data/LabelHand',\n",
       "   'train_split': '../data/sen1floods11_v1.1/splits/flood_train_data.txt',\n",
       "   'val_split': '../data/sen1floods11_v1.1/splits/flood_valid_data.txt',\n",
       "   'test_split': '../data/sen1floods11_v1.1/splits/flood_test_data.txt',\n",
       "   'image_grep': {'S2L1C': '*_S2Hand.tif', 'S1GRD': '*_S1Hand.tif'},\n",
       "   'label_grep': '*_LabelHand.tif',\n",
       "   'no_label_replace': -1,\n",
       "   'no_data_replace': 0,\n",
       "   'num_classes': 2,\n",
       "   'means': {'S2L1C': [2357.089,\n",
       "     2137.385,\n",
       "     2018.788,\n",
       "     2082.986,\n",
       "     2295.651,\n",
       "     2854.537,\n",
       "     3122.849,\n",
       "     3040.56,\n",
       "     3306.481,\n",
       "     1473.847,\n",
       "     506.07,\n",
       "     2472.825,\n",
       "     1838.929],\n",
       "    'S2L2A': [1390.458,\n",
       "     1503.317,\n",
       "     1718.197,\n",
       "     1853.91,\n",
       "     2199.1,\n",
       "     2779.975,\n",
       "     2987.011,\n",
       "     3083.234,\n",
       "     3132.22,\n",
       "     3162.988,\n",
       "     2424.884,\n",
       "     1857.648],\n",
       "    'S1GRD': [-12.599, -20.293],\n",
       "    'S1RTC': [-10.93, -17.329],\n",
       "    'RGB': [87.271, 80.931, 66.667],\n",
       "    'DEM': [670.665]},\n",
       "   'stds': {'S2L1C': [1624.683,\n",
       "     1675.806,\n",
       "     1557.708,\n",
       "     1833.702,\n",
       "     1823.738,\n",
       "     1733.977,\n",
       "     1732.131,\n",
       "     1679.732,\n",
       "     1727.26,\n",
       "     1024.687,\n",
       "     442.165,\n",
       "     1331.411,\n",
       "     1160.419],\n",
       "    'S2L2A': [2106.761,\n",
       "     2141.107,\n",
       "     2038.973,\n",
       "     2134.138,\n",
       "     2085.321,\n",
       "     1889.926,\n",
       "     1820.257,\n",
       "     1871.918,\n",
       "     1753.829,\n",
       "     1797.379,\n",
       "     1434.261,\n",
       "     1334.311],\n",
       "    'S1GRD': [5.195, 5.89],\n",
       "    'S1RTC': [4.391, 4.459],\n",
       "    'RGB': [58.767, 47.663, 42.631],\n",
       "    'DEM': [951.272]},\n",
       "   'train_transform': [{'class_path': 'albumentations.D4'},\n",
       "    {'class_path': 'ToTensorV2'}]}},\n",
       " 'model': {'class_path': 'terratorch.tasks.SemanticSegmentationTask',\n",
       "  'init_args': {'model_factory': 'EncoderDecoderFactory',\n",
       "   'model_args': {'backbone': 'terramind_v1_base',\n",
       "    'backbone_pretrained': True,\n",
       "    'backbone_modalities': ['S2L1C', 'S1GRD'],\n",
       "    'backbone_merge_method': 'mean',\n",
       "    'necks': [{'name': 'SelectIndices', 'indices': [2, 5, 8, 11]},\n",
       "     {'name': 'ReshapeTokensToImage', 'remove_cls_token': False},\n",
       "     {'name': 'LearnedInterpolateToPyramidal'}],\n",
       "    'decoder': 'UNetDecoder',\n",
       "    'decoder_channels': [512, 256, 128, 64],\n",
       "    'head_dropout': 0.1,\n",
       "    'num_classes': 2},\n",
       "   'loss': 'dice',\n",
       "   'ignore_index': -1,\n",
       "   'freeze_backbone': True,\n",
       "   'freeze_decoder': False,\n",
       "   'class_names': ['Others', 'Flood']}},\n",
       " 'optimizer': {'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 2e-05}},\n",
       " 'lr_scheduler': {'class_path': 'ReduceLROnPlateau',\n",
       "  'init_args': {'monitor': 'val/loss', 'factor': 0.5, 'patience': 5}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check config file.\n",
    "import yaml\n",
    "with open('../configs/terramind_v1_base_sen1floods11.yaml') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0453502fb0bf62",
   "metadata": {},
   "source": [
    "# TerraTorch model registry\n",
    "\n",
    "TerraTorch includes its own backbone registry with many EO FMs. It also includes meta registries for all model components that include other sources like timm image models or SMP decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970183baaea88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.registry import BACKBONE_REGISTRY, TERRATORCH_BACKBONE_REGISTRY, TERRATORCH_DECODER_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d15609",
   "metadata": {},
   "source": [
    "**Note:** TiM models are using the Thinking-in-Modalities approach, see our [paper](https://arxiv.org/abs/2504.11171) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4109f8f262cc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all TerraMind v1 backbones.  \n",
    "[backbone   \n",
    " for backbone in TERRATORCH_BACKBONE_REGISTRY\n",
    " if 'terramind_v1' in backbone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51fdde4e1e5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available decoders. We use the UNetDecoder in this example.\n",
    "list(TERRATORCH_DECODER_REGISTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91461a",
   "metadata": {},
   "source": [
    "# TerraMind Training Configuration Overview\n",
    "\n",
    "This section provides a detailed explanation of the parameters used for configuring a TerraMind model fine-tuning job.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 **Core Setup & Identifiers**\n",
    "\n",
    "-   **`root_dir`**:\n",
    "    -   **Explanation**: Specifies the absolute base directory where essential data and configurations are located, especially relevant in cloud environments like SageMaker.\n",
    "    -   **Example**: `/opt/ml/data/`\n",
    "    -   **Usage**: Defines the root for accessing input data and often for storing outputs if not otherwise specified.\n",
    "\n",
    "-   **`identifier`**:\n",
    "    -   **Explanation**: A user-defined string that serves as a unique prefix for naming all artifacts generated during the training and deployment process. This includes model checkpoints, logs, and output files.\n",
    "    -   **Example**: `sen1floods_experiment_001`\n",
    "    -   **Usage**: Helps in organizing and distinguishing different experimental runs.\n",
    "\n",
    "-   **`usecase`**:\n",
    "    -   **Explanation**: A string defining the specific Earth observation application or dataset the model is being trained or fine-tuned for.\n",
    "    -   **Example**: `terramind_base_sen1floods11`\n",
    "    -   **Usage**: Used for naming conventions, potentially for loading specific configurations, and organizing outputs related to this particular task.\n",
    "\n",
    "-   **`data_path`**:\n",
    "    -   **Explanation**: The local file system path where the input dataset resides before any processing or uploading to cloud storage.\n",
    "    -   **Example**: `../data/sen1floods11_v1.1`\n",
    "    -   **Usage**: Primarily used to locate raw data files for preprocessing, statistics calculation, or transferring to a training environment.\n",
    "\n",
    "-   **`output_folder`**:\n",
    "    -   **Explanation**: The base directory where all output artifacts from the training process (like model checkpoints, logs, and predictions) will be stored. It's constructed using the `root_dir`.\n",
    "    -   **Example**: `/opt/ml/data/output` (derived from `root_dir`)\n",
    "    -   **Usage**: Central location for all generated training outputs.\n",
    "\n",
    "-   **`default_root_dir`**:\n",
    "    -   **Explanation**: A more specific root directory, typically within the `output_folder`, tailored to the current `usecase`. It serves as the base for storing processed data, data splits, and other use-case-specific files.\n",
    "    -   **Example**: `/opt/ml/data/output/terramind_base_sen1floods11/`\n",
    "    -   **Usage**: Organizes data specific to a particular use case, like processed images and labels.\n",
    "\n",
    "-   **`checkpoint_path`**:\n",
    "    -   **Explanation**: The specific directory where model checkpoints (saved states of the model during training) are stored. This path is usually within the `output_folder`.\n",
    "    -   **Example**: `/opt/ml/data/output/burnscars/checkpoints` (*Note: The example path seems to reference \"burnscars\" while the usecase is \"sen1floods11\". Ensure consistency in actual use.*)\n",
    "    -   **Usage**: Essential for resuming training or for deploying a trained model from a specific point.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Data Standardization & Normalization**\n",
    "\n",
    "-   **`data_mean`**:\n",
    "    -   **Explanation**: A dictionary where keys are modality names (e.g., \"S2L1C\", \"S1GRD\") and values are lists of mean values for each band/channel of that modality. These values are crucial for normalizing the input data (subtracting the mean). Using pre-training values is common, but dataset-specific statistics can also be computed and used.\n",
    "    -   **Example**: `{\"S1GRD\": [-12.599, -20.293], \"S2L1C\": [2357.089, ...]}`\n",
    "    -   **Usage**: Applied during data preprocessing to standardize the input features, which typically helps in model convergence and performance.\n",
    "\n",
    "-   **`data_stds`**:\n",
    "    -   **Explanation**: Similar to `data_mean`, this dictionary holds the standard deviation values for each band/channel of the specified modalities. Used for scaling the input data after mean subtraction (dividing by standard deviation).\n",
    "    -   **Example**: `{\"S1GRD\": [5.195, 5.890], \"S2L1C\": [1624.683, ...]}`\n",
    "    -   **Usage**: Works in conjunction with `data_mean` for Z-score normalization of the input data.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **Training Job Parameters**\n",
    "\n",
    "-   **`batch_size`**:\n",
    "    -   **Explanation**: The number of data samples (e.g., image patches) that are processed by the model in one forward/backward pass during training.\n",
    "    -   **Example**: `4`\n",
    "    -   **Usage**: Affects memory consumption (GPU VRAM) and training stability. Larger batch sizes can lead to faster training per epoch but require more memory.\n",
    "\n",
    "-   **`num_workers`**:\n",
    "    -   **Explanation**: The number of CPU worker processes assigned to load and preprocess data in parallel while the GPU is busy with model computations.\n",
    "    -   **Example**: `2`\n",
    "    -   **Usage**: Can significantly speed up data loading, preventing bottlenecks if data preprocessing is complex. The optimal value depends on CPU cores and I/O capabilities.\n",
    "\n",
    "-   **`num_classes`**:\n",
    "    -   **Explanation**: The total number of distinct classes the model is expected to predict in a segmentation task. For binary segmentation (e.g., flood vs. no-flood), this would be 2.\n",
    "    -   **Example**: `2`\n",
    "    -   **Usage**: Defines the output dimension of the final layer in the segmentation model.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛰️ **Data Modality & Structure Configuration**\n",
    "\n",
    "-   **`modalities`**:\n",
    "    -   **Explanation**: A list of strings specifying the names of the data modalities (e.g., sensor types or data products) that will be used as input to the model.\n",
    "    -   **Example**: `[\"S2L1C\", \"S1GRD\"]`\n",
    "    -   **Usage**: Informs the data loader which types of data to load and process for each sample.\n",
    "\n",
    "-   **`rgb_modality`**:\n",
    "    -   **Explanation**: A string specifying which of the input `modalities` should be considered the primary source for generating RGB visualizations or plots. If not provided, it typically defaults to the first modality in the `modalities` list.\n",
    "    -   **Example**: `\"S2L1C\"`\n",
    "    -   **Usage**: Used by visualization tools or callbacks to display image samples during or after training.\n",
    "\n",
    "-   **`rgb_indices`**:\n",
    "    -   **Explanation**: A list of integers indicating the band positions (0-indexed) within the `rgb_modality` that correspond to the Red, Green, and Blue channels, respectively, for creating true or false-color composite images.\n",
    "    -   **Example**: `[3, 2, 1]` (for S2L1C, this would typically be B4, B3, B2 for a standard RGB view)\n",
    "    -   **Usage**: Essential for correct visualization of multi-band satellite imagery.\n",
    "\n",
    "-   **`train_data_root`**, **`val_data_root`**, **`test_data_root`**:\n",
    "    -   **Explanation**: Dictionaries where keys are modality names and values are the file paths to the root directories containing the training, validation, and testing image data for each respective modality.\n",
    "    -   **Example (`train_data_root`)**: `{\"S2L1C\": \"/opt/ml/data/output/terramind_base_sen1floods11/S2L1CHand\", \"S1GRD\": \"...\"}`\n",
    "    -   **Usage**: Tells the data loader where to find the image files for each data split and modality.\n",
    "\n",
    "-   **`train_label_data_root`**, **`val_label_data_root`**, **`test_label_data_root`**:\n",
    "    -   **Explanation**: Strings specifying the file paths to the root directories containing the corresponding label (ground truth segmentation masks) data for the training, validation, and testing sets.\n",
    "    -   **Example (`train_label_data_root`)**: `/opt/ml/data/output/terramind_base_sen1floods11/LabelHand`\n",
    "    -   **Usage**: Directs the data loader to the location of ground truth masks.\n",
    "\n",
    "-   **`train_split`**, **`val_split`**, **`test_split`**:\n",
    "    -   **Explanation**: File paths to text files that define the data splits. These files typically list the names or identifiers of the samples belonging to the training, validation, and testing sets, respectively. This is useful when all image/label files are stored in common directories.\n",
    "    -   **Example (`train_split`)**: `/opt/ml/data/output/terramind_base_sen1floods11/splits/flood_train_data.txt`\n",
    "    -   **Usage**: Ensures consistent and reproducible data splits across different runs.\n",
    "\n",
    "-   **`img_grep`**:\n",
    "    -   **Explanation**: A dictionary where keys are modality names and values are glob patterns (wildcard expressions) used to identify and filter image files for each modality within their respective data root directories.\n",
    "    -   **Example**: `{\"S2L1C\": \"*_S2Hand.tif\", \"S1GRD\": \"*_S1Hand.tif\"}`\n",
    "    -   **Usage**: Helps in selectively loading files that match a specific naming convention for each modality.\n",
    "\n",
    "-   **`label_grep`**:\n",
    "    -   **Explanation**: A glob pattern used to identify and filter label files (segmentation masks) within their data root directory.\n",
    "    -   **Example**: `*_LabelHand.tif`\n",
    "    -   **Usage**: Selectively loads label files matching a specific naming convention.\n",
    "\n",
    "-   **`dataset_bands`**:\n",
    "    -   **Explanation**: An optional dictionary where keys are modality names and values are lists of strings representing the names of all available bands in the original dataset files for that modality. This provides context for band selection.\n",
    "    -   **Example**: `{\"S1GRD\": [\"VV\", \"VH\"]}`\n",
    "    -   **Usage**: Used in conjunction with `output_bands` to specify which bands to select from the source files.\n",
    "\n",
    "-   **`output_bands`**:\n",
    "    -   **Explanation**: An optional dictionary, similar to `dataset_bands`. It specifies the subset of bands (by name, corresponding to names in `dataset_bands`) that should actually be extracted and used as input to the model for each modality. If not provided for a modality, all bands are typically used.\n",
    "    -   **Example**: `{\"S1GRD\": [\"VV\", \"VH\"]}` (Here, selecting both available S1GRD bands. Could be `{\"S1GRD\": [\"VV\"]}` to use only VV).\n",
    "    -   **Usage**: Allows for experimentation with different band combinations without modifying the original data files. The `data_mean` and `data_stds` must align with these selected `output_bands`.\n",
    "\n",
    "-   **`no_label_replace`**:\n",
    "    -   **Explanation**: A value used to replace NaN (Not a Number) or missing values in the label data (ground truth masks). The default of `-1` is often used because it can be configured to be ignored by the loss function and evaluation metrics.\n",
    "    -   **Example**: `-1`\n",
    "    -   **Usage**: Handles missing or invalid pixels in ground truth data.\n",
    "\n",
    "-   **`no_data_replace`**:\n",
    "    -   **Explanation**: A value used to replace NaN or missing values in the input image data across all modalities.\n",
    "    -   **Example**: `0`\n",
    "    -   **Usage**: Ensures that the model receives valid numerical inputs by handling missing pixels in the source imagery.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **Model Architecture & Training Strategy**\n",
    "\n",
    "-   **`terramind_backbone`**:\n",
    "    -   **Explanation**: A string specifying the pre-trained TerraMind foundation model architecture to be used as the backbone (feature extractor) for the fine-tuning task. Different versions offer trade-offs in size, speed, and performance.\n",
    "    -   **Choices**: `'terramind_v1_base'`, `'terramind_v1_base_tim'`, `'terramind_v1_large'`, `'terramind_v1_large_tim'`\n",
    "    -   **Example**: `'terramind_v1_base'`\n",
    "    -   **Usage**: Critical choice that determines the core feature extraction capabilities of the model.\n",
    "\n",
    "-   **`max_epochs`**:\n",
    "    -   **Explanation**: The maximum number of times the entire training dataset will be passed forward and backward through the neural network.\n",
    "    -   **Example**: `100`\n",
    "    -   **Usage**: Controls the total duration of training. Training might stop earlier if early stopping criteria are met.\n",
    "\n",
    "-   **`indices`**:\n",
    "    -   **Explanation**: A list of integers specifying which Transformer blocks (by their 0-indexed layer number) from the TerraMind backbone will provide their output feature embeddings to the decoder part of the model. This enables the decoder to use multi-scale features. The appropriate indices depend on the chosen `terramind_backbone`'s architecture (e.g., 'base' vs. 'large' versions have different total numbers of layers).\n",
    "    -   **Conditional Logic**: The provided code snippet shows logic to set these indices based on whether a 'base' or 'large' backbone is selected.\n",
    "        -   For 'base' models: `[2, 5, 8, 11]`\n",
    "        -   For 'large' models: `[5, 11, 17, 23]`\n",
    "    -   **Usage**: Crucial for how the decoder reconstructs segmentation maps from backbone features.\n",
    "\n",
    "-   **`model_factory`**:\n",
    "    -   **Explanation**: A string specifying the factory class responsible for constructing the overall model architecture. The `EncoderDecoderFactory` typically combines the chosen backbone with a neck (optional), a decoder, and a task-specific head.\n",
    "    -   **Example**: `\"EncoderDecoderFactory\"`\n",
    "    -   **Usage**: Defines the high-level structure for assembling the model components.\n",
    "\n",
    "-   **`remove_cls_token`**:\n",
    "    -   **Explanation**: A boolean indicating whether the special CLS (classification) token, often used in Vision Transformer architectures for image-level classification tasks, should be removed or ignored when using the backbone for dense prediction tasks like segmentation. TerraMind models are often trained without relying on a CLS token for segmentation.\n",
    "    -   **Example**: `False` (*The comment says \"TerraMind is trained without CLS token, which neads to be specified.\" This suggests it should likely be `True` if the backbone was pre-trained without a CLS token for segmentation and the fine-tuning setup needs to account for that. Clarify based on TerraMind's specific requirements.*)\n",
    "    -   **Usage**: Ensures compatibility between the backbone's output and the decoder's input for segmentation.\n",
    "\n",
    "-   **`freeze_backbone`**:\n",
    "    -   **Explanation**: A boolean value. If `True`, the weights of the pre-trained TerraMind backbone are frozen and not updated during fine-tuning. Only the weights of the newly added decoder and head are trained.\n",
    "    -   **Example**: `True`\n",
    "    -   **Usage**: Can speed up fine-tuning and reduce memory usage, especially for demonstrations or when fine-tuning data is very limited. However, for best performance, setting this to `False` (fine-tuning the entire backbone or parts of it) is generally recommended.\n",
    "\n",
    "-   **`freeze_decoder`**:\n",
    "    -   **Explanation**: A boolean value. If `True`, the weights of the decoder part of the model are frozen. This is rarely set to `True` because the decoder is typically randomly initialized or adapted from a different task and needs to be trained for the specific fine-tuning use case.\n",
    "    -   **Example**: `False`\n",
    "    -   **Usage**: Should generally be `False` to allow the decoder to learn its task.\n",
    "\n",
    "-   **`class_names`**:\n",
    "    -   **Explanation**: An optional list of strings that provide human-readable names for each class index. The order of names should correspond to the class indices (e.g., class 0, class 1, ...).\n",
    "    -   **Example**: `[\"Others\", \"Water\"]` (for `num_classes = 2`)\n",
    "    -   **Usage**: Useful for logging, creating legends in visualizations, and interpreting model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/opt/ml/data'\n",
    "\n",
    "# Parameters to modify\n",
    "identifier = <identifier>\n",
    "\n",
    "# Usecase definition\n",
    "usecase = \"terramind_base_sen1floods11\"\n",
    "\n",
    "#local data path\n",
    "data_path = '../data/sen1floods11_v1.1'\n",
    "\n",
    "# Output base directory\n",
    "output_folder = f\"{root_dir}/output\"\n",
    "\n",
    "# Default root dir for all data and splits\n",
    "default_root_dir = f\"{output_folder}/terramind_base_sen1floods11/\"\n",
    "\n",
    "# Path to save checkpoints in\n",
    "checkpoint_path = f\"{output_folder}/{usecase}/checkpoints\"\n",
    "\n",
    "# Define standardization values. We use the pre-training values here and providing the additional modalities is not a problem, \n",
    "# which makes it simple to experiment with different modality combinations. \n",
    "# Alternatively, use the dataset statistics that you can generate using \n",
    "# `terratorch compute_statistics -c config.yaml` (requires concat_bands: true for this multimodal datamodule).\n",
    "data_mean = {\n",
    "    \"S2L1C\": [2357.089, 2137.385, 2018.788, 2082.986, 2295.651, 2854.537, 3122.849, 3040.560, 3306.481, 1473.847, 506.070, 2472.825, 1838.929],\n",
    "    \"S2L2A\": [1390.458, 1503.317, 1718.197, 1853.910, 2199.100, 2779.975, 2987.011, 3083.234, 3132.220, 3162.988, 2424.884, 1857.648],\n",
    "    \"S1GRD\": [-12.599, -20.293],\n",
    "    \"S1RTC\": [-10.93, -17.329],\n",
    "    \"RGB\": [87.271, 80.931, 66.667],\n",
    "    \"DEM\": [670.665]\n",
    "}\n",
    "\n",
    "data_stds = {\n",
    "    \"S2L1C\": [1624.683, 1675.806, 1557.708, 1833.702, 1823.738, 1733.977, 1732.131, 1679.732, 1727.26, 1024.687, 442.165, 1331.411, 1160.419],\n",
    "    \"S2L2A\": [2106.761, 2141.107, 2038.973, 2134.138, 2085.321, 1889.926, 1820.257, 1871.918, 1753.829, 1797.379, 1434.261, 1334.311],\n",
    "    \"S1GRD\": [5.195, 5.890],\n",
    "    \"S1RTC\": [4.391, 4.459],\n",
    "    \"RGB\": [58.767, 47.663, 42.631],\n",
    "    \"DEM\": [951.272],\n",
    "}\n",
    "\n",
    "# Batches of examples to use at a time\n",
    "batch_size = 4\n",
    "\n",
    "# Number of training job workers\n",
    "num_workers = 2\n",
    "\n",
    "# Number of classes for segmentation\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "modalities = [\"S2L1C\", \"S1GRD\"]\n",
    "rgb_modality = \"S2L1C\"  # Used for plotting. Defaults to the first modality if not provided.\n",
    "rgb_indices = [3,2,1]  # RGB channel positions in the rgb_modality.\n",
    "\n",
    "# Define data paths as dicts using the modality names as keys.\n",
    "train_data_root = {\n",
    "    \"S2L1C\": f\"{root_dir}/data/S2L1CHand\",\n",
    "    \"S1GRD\": f\"{root_dir}/data/S1GRDHand\",\n",
    "}\n",
    "train_label_data_root = f\"{root_dir}/data/LabelHand\"\n",
    "val_data_root = {\n",
    "    \"S2L1C\": f\"{root_dir}/data/S2L1CHand\",\n",
    "    \"S1GRD\": f\"{root_dir}/data/S1GRDHand\",\n",
    "}\n",
    "val_label_data_root = f\"{root_dir}/data/LabelHand\"\n",
    "test_data_root = {\n",
    "    \"S2L1C\": f\"{root_dir}/data/S2L1CHand\",\n",
    "    \"S1GRD\": f\"{root_dir}/data/S1GRDHand\",\n",
    "}\n",
    "test_label_data_root = f\"{root_dir}/data/LabelHand\"\n",
    "\n",
    "# Define split files because all samples are saved in the same folder.\n",
    "train_split = f\"{root_dir}/splits/flood_train_data.txt\"\n",
    "val_split = f\"{root_dir}/splits/flood_valid_data.txt\"\n",
    "test_split = f\"{root_dir}/splits/flood_test_data.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# Define suffix, again using dicts.\n",
    "img_grep = {\n",
    "    \"S2L1C\": \"*_S2Hand.tif\",\n",
    "    \"S1GRD\": \"*_S1Hand.tif\",\n",
    "}\n",
    "label_grep = \"*_LabelHand.tif\"\n",
    "\n",
    "# With TerraTorch, you can select a subset of the dataset bands as model inputs by providing dataset_bands (all bands in the data) and output_bands (selected bands). This setting is optional for all modalities and needs to be provided as dicts.\n",
    "# Here is an example for with S-1 GRD. You could change the output to [\"VV\"] to only train on the first band. Note that means and stds must be aligned with the output_bands (equal length of values). \n",
    "dataset_bands = {\n",
    "    \"S1GRD\": [\"VV\", \"VH\"]\n",
    "}\n",
    "output_bands = {\n",
    "    \"S1GRD\": [\"VV\", \"VH\"]\n",
    "}\n",
    "\n",
    "no_label_replace = -1  # Replace NaN labels. defaults to -1 which is ignored in the loss and metrics.\n",
    "no_data_replace = 0  # Replace NaN data\n",
    "\n",
    "terramind_backbone = 'terramind_v1_base' # choice of 'terramind_v1_base', 'terramind_v1_base_tim', 'terramind_v1_large', 'terramind_v1_large_tim'\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "indices = [5, 11, 17, 23]\n",
    "if 'terramind_v1_base' in terramind_backbone:\n",
    "    indices = [2, 5, 8, 11]  # indices for terramind_v1_base\n",
    "elif 'terramind_v1_large' in terramind_backbone: \n",
    "    indices = [5, 11, 17, 23]  # indices for terramind_v1_large\n",
    "\n",
    "# Define standardization values. We use the pre-training values here and providing the additional modalities is not a problem, which makes it simple to experiment with different modality combinations. Alternatively, use the dataset statistics that you can generate using `terratorch compute_statistics -c config.yaml` (requires concat_bands: true for this multimodal datamodule).\n",
    "data_mean = {\n",
    "    \"S2L1C\": [2357.089, 2137.385, 2018.788, 2082.986, 2295.651, 2854.537, 3122.849, 3040.560, 3306.481, 1473.847, 506.070, 2472.825, 1838.929],\n",
    "    \"S2L2A\": [1390.458, 1503.317, 1718.197, 1853.910, 2199.100, 2779.975, 2987.011, 3083.234, 3132.220, 3162.988, 2424.884, 1857.648],\n",
    "    \"S1GRD\": [-12.599, -20.293],\n",
    "    \"S1RTC\": [-10.93, -17.329],\n",
    "    \"RGB\": [87.271, 80.931, 66.667],\n",
    "    \"DEM\": [670.665]\n",
    "}\n",
    "\n",
    "data_stds = {\n",
    "    \"S2L1C\": [1624.683, 1675.806, 1557.708, 1833.702, 1823.738, 1733.977, 1732.131, 1679.732, 1727.26, 1024.687, 442.165, 1331.411, 1160.419],\n",
    "    \"S2L2A\": [2106.761, 2141.107, 2038.973, 2134.138, 2085.321, 1889.926, 1820.257, 1871.918, 1753.829, 1797.379, 1434.261, 1334.311],\n",
    "    \"S1GRD\": [5.195, 5.890],\n",
    "    \"S1RTC\": [4.391, 4.459],\n",
    "    \"RGB\": [58.767, 47.663, 42.631],\n",
    "    \"DEM\": [951.272],\n",
    "}\n",
    "\n",
    "model_factory=\"EncoderDecoderFactory\"  # Combines a backbone with necks, the decoder, and a head\n",
    "\n",
    "remove_cls_token = False # TerraMind is trained without CLS token, which neads to be specified.\n",
    "\n",
    "freeze_backbone = True # Only used to speed up fine-tuning in this demo, we highly recommend fine-tuning the backbone for the best performance. \n",
    "freeze_decoder = False  # Should be false in most cases as the decoder is randomly initialized.\n",
    "class_names = [\"Others\", \"Water\"]  # optionally define class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer specific configurations\n",
    "config['trainer']['default_root_dir'] = default_root_dir\n",
    "config['trainer']['max_epochs'] = max_epochs\n",
    "config['trainer']['callbacks'][-1]['init_args']['dirpath'] = checkpoint_path\n",
    "\n",
    "# Model specific configurations\n",
    "config['model']['init_args']['model_args']['necks'][0]['indices'] = indices\n",
    "config['model']['init_args']['model_args']['backbone'] = terramind_backbone\n",
    "config['model']['init_args']['model_factory'] = model_factory\n",
    "config['model']['init_args']['class_names'] = class_names\n",
    "config['model']['init_args']['freeze_decoder'] = freeze_decoder\n",
    "config['model']['init_args']['freeze_backbone'] = freeze_backbone\n",
    "\n",
    "for index, neck in enumerate(config['model']['init_args']['model_args']['necks']):\n",
    "    if neck['name'] == 'ReshapeTokensToImage':\n",
    "        config['model']['init_args']['model_args']['necks'][index]['remove_cls_token'] = remove_cls_token\n",
    "    elif neck['name'] == 'LearnedInterpolateToPyramidal':\n",
    "        # Some decoders like UNet or UperNet expect hierarchical features. \n",
    "        # Therefore, we need to learn a upsampling for the intermediate embedding layers when using a ViT like TerraMind.\n",
    "        continue\n",
    "\n",
    "\n",
    "# Data specific configurations\n",
    "config['data']['init_args']['train_data_root'] = train_data_root\n",
    "config['data']['init_args']['test_data_root'] = test_data_root\n",
    "config['data']['init_args']['val_data_root'] = val_data_root\n",
    "\n",
    "config['data']['init_args']['train_label_data_root'] = train_label_data_root\n",
    "config['data']['init_args']['val_label_data_root'] = val_label_data_root\n",
    "config['data']['init_args']['test_label_data_root'] = test_label_data_root\n",
    "\n",
    "config['data']['init_args']['train_split'] = train_split\n",
    "config['data']['init_args']['val_split'] = val_split\n",
    "config['data']['init_args']['test_split'] = test_split\n",
    "\n",
    "config['data']['init_args']['modalities'] = modalities\n",
    "\n",
    "config['data']['init_args']['label_grep'] = label_grep\n",
    "config['data']['init_args']['image_grep'] = img_grep\n",
    "\n",
    "config['data']['init_args']['num_classes'] = num_classes\n",
    "config['data']['init_args']['no_data_replace'] = no_data_replace\n",
    "config['data']['init_args']['no_label_replace'] = no_label_replace\n",
    "\n",
    "config['data']['init_args']['means'] = data_mean \n",
    "config['data']['init_args']['stds'] = data_stds\n",
    "\n",
    "config['data']['init_args']['batch_size'] = batch_size\n",
    "config['data']['init_args']['num_workers'] = num_workers\n",
    "\n",
    "config['data']['init_args']['num_classes'] = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename configuration file name to user specific filename\n",
    "import os\n",
    "\n",
    "config_filename = f\"{identifier}-flood_terramind.yaml\"\n",
    "config_filepath = f\"../configs/{config_filename}\"\n",
    "with open(config_filepath, 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)\n",
    "\n",
    "# Upload config files to s3 bucket\n",
    "configs = sagemaker_session.upload_data(path=config_filepath, bucket=BUCKET_NAME, key_prefix='data/configs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## Sen1Floods11 Dataset\n",
    "\n",
    "Lets start with analysing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f22dc984ead544",
   "metadata": {},
   "source": [
    "TerraTorch provides generic data modules that work directly with PyTorch Lightning.\n",
    "\n",
    "Sen1Floods11 is a multimodal dataset that provides Sentinel-2 L2A and Sentinel-1 GRD data. \n",
    "Therefore, we are using the `GenericMultiModalDataModule`. \n",
    "This module is similar to the `GenericNonGeoSegmentationDataModule`, which is used for standard segmentation tasks.\n",
    "However, the data roots, `img_grep` are other settings are provided as dict to account for the multimodal inputs. You find all settings in the [documentation](https://ibm.github.io/terratorch/stable/generic_datamodules/). \n",
    "In a Lightning config, the data module is defined with the `data` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f9c2593c90b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting both modalities\n",
    "def plot_sample(sample):\n",
    "    s1 = sample['image']['S1GRD']\n",
    "    s2 = sample['image']['S2L1C']\n",
    "    mask = sample['mask']\n",
    "    \n",
    "    # Scaling data. Using -30 to 0 scaling for S-1 and 0 - 2000 for S-2. S-1 is visualized as [VH, VV, VH]\n",
    "    s1 = (s1.clip(-30, 0) / 30 + 1) * 255\n",
    "    s2 = (s2.clip(0, 2000) / 2000) * 255\n",
    "    s1_rgb = np.stack([s1[1], s1[0], s1[1]], axis=0).astype(np.uint8).transpose(1,2,0)\n",
    "    s2_rgb = s2[[3,2,1]].astype(np.uint8).transpose(1,2,0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax[0].imshow(s1_rgb)\n",
    "    ax[0].set_title('S-1 GRD')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(s2_rgb)\n",
    "    ax[1].set_title('S-2 GRD')\n",
    "    ax[1].axis('off')   \n",
    "    ax[2].imshow(mask, vmin=-1, vmax=1, interpolation='nearest')\n",
    "    ax[2].set_title('Mask')\n",
    "    ax[2].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb92b1d0599a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some samples\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "samples = glob(f\"{data_path}/data/S*/*.tif\")\n",
    "samples = random.sample(samples, 3)\n",
    "\n",
    "for sample in samples:\n",
    "    s1_file = sample.replace('S2L1CHand', 'S1GRDHand').replace('S2Hand', 'S1Hand')\n",
    "    s2_file = sample.replace('S1GRDHand', 'S2L1CHand').replace('S1Hand', 'S2Hand')\n",
    "    mask_file = sample.replace('S1GRDHand', 'LabelHand').replace('S2L1CHand', 'LabelHand').replace('S2Hand', 'LabelHand').replace('S1Hand', 'LabelHand')\n",
    "    updated_values = {\n",
    "        'image': {\n",
    "            'S1GRD': rasterio.open(s1_file).read(),\n",
    "            'S2L1C': rasterio.open(s2_file).read(),\n",
    "        },\n",
    "        'mask': rasterio.open(mask_file).read()[0]\n",
    "    }\n",
    "    plot_sample(updated_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a30ddef8ed5a",
   "metadata": {},
   "source": [
    "# Fine-tune TerraMind via PyTorch Lightning\n",
    "\n",
    "With TerraTorch, we can use standard Lightning components for the fine-tuning.\n",
    "These include callbacks and the trainer class.\n",
    "TerraTorch provides EO-specific tasks that define the training and validation steps.\n",
    "In this case, we are using the `SemanticSegmentationTask`.\n",
    "We refer to the [TerraTorch paper](https://arxiv.org/abs/2503.20563) for a detailed explanation of the TerraTorch tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup variables for training using sagemaker\n",
    "\n",
    "name = f'{identifier}-sagemaker'\n",
    "role = get_execution_role()\n",
    "input_s3_uri = f\"s3://{BUCKET_NAME}/data\"\n",
    "model_name = f\"{identifier}-workshop.ckpt\",\n",
    "model_path = f\"{root_dir}/{usecase}/checkpoints\"\n",
    "\n",
    "environment_variables = {\n",
    "    'CONFIG_FILE': f\"{root_dir}/configs/{config_filename}\",\n",
    "    'MODEL_DIR': model_path,\n",
    "    'MODEL_NAME': model_name,\n",
    "    'S3_URL': input_s3_uri,\n",
    "    'ROLE_ARN': role,\n",
    "    'ROLE_NAME': role.split('/')[-1],\n",
    "    'EVENT_TYPE': usecase,\n",
    "    'SPLITS': 'splits,configs,data/LabelHand,data/S1GRDHand,data/S2L1CHand',\n",
    "    'VERSION': 'v1'\n",
    "}\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_container_url = f'{account_id}.dkr.ecr.us-west-2.amazonaws.com/eo_training:latest'\n",
    "sagemaker_role = get_execution_role().split('/')[-1]\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "instance_count = 1\n",
    "memory_volume = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ac613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Establish an estimator (model) using sagemaker and the configurations from the previous cell.\n",
    "estimator = Estimator(image_uri=ecr_container_url,\n",
    "                      role=get_execution_role(),\n",
    "                      base_job_name=name,\n",
    "                      instance_count=1,\n",
    "                      environment=environment_variables,\n",
    "                      instance_type=instance_type)\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save important values in a file for reuse.\n",
    "export_values = {\n",
    "    'identifier': identifier,\n",
    "    'model_name': model_name,\n",
    "    'config_filename': config_filename,\n",
    "    'bucket_name': BUCKET_NAME\n",
    "}\n",
    "\n",
    "with open('../variables.yaml', 'w') as variable_export:\n",
    "    yaml.dump(export_values, variable_export, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bebdb7370a174",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a77263-5308-4781-a17f-a35e62ca1875",
   "metadata": {
    "id": "35a77263-5308-4781-a17f-a35e62ca1875",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's test the fine-tuned model\n",
    "best_ckpt_path = f\"{output_folder}/terramind_base_sen1floods11/checkpoints/best-mIoU.ckpt\"\n",
    "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
    "\n",
    "# Note: This demo only trains for 5 epochs by default, which does not result in good test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
    "outputId": "c7c06228-e634-4608-cb0e-617d003fea46"
   },
   "outputs": [],
   "source": [
    "# Now we can use the model for predictions and plotting\n",
    "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "    best_ckpt_path,\n",
    "    model_factory=model.hparams.model_factory,\n",
    "    model_args=model.hparams.model_args,\n",
    ")\n",
    "\n",
    "test_loader = datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "    images = batch[\"image\"]\n",
    "    for mod, value in images.items():\n",
    "        images[mod] = value.to(model.device)\n",
    "    masks = batch[\"mask\"].numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(4):\n",
    "    sample = {\n",
    "        \"image\": batch[\"image\"][\"S2L1C\"][i].cpu(),\n",
    "        \"mask\": batch[\"mask\"][i],\n",
    "        \"prediction\": preds[i],\n",
    "    }\n",
    "    test_dataset.plot(sample)\n",
    "    plt.show()\n",
    "    \n",
    "# Note: This demo only trains for 5 epochs by default, which does not result in good predictions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
