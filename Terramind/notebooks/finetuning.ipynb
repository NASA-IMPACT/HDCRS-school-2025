{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bacc318390456b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook focuses on finetuning the terramind model to identify floods in a sentinel scene. The main take aways from this notebook will be as follows:\n",
    "1. Learn how to use Terratorch to finetune terramind for floods in sentinel scene.\n",
    "2. Understand the effects of spefic parameters in training and hardware utilization.\n",
    "\n",
    "**Note:** The entirety of this notebook is tuned to work well in a sagemaker environment. Sagemaker Training Job will be used to train the models. If you are interested in running this in a colab environment, please leverage [ESA NASA Foundation Model Workshop](https://github.com/NASA-IMPACT/ESA-NASA-workshop-2025/tree/main/Track%201%20(EO)/TerraMind) materials.\n",
    "\n",
    "## Setup\n",
    "1. Go to \"Kernel\"\n",
    "2. Select \"terramind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "import sagemaker\n",
    "import terratorch\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from datetime import time\n",
    "from glob import glob\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b65b8e7cd7d65",
   "metadata": {},
   "source": [
    "3. Download the dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dw5-9A4A4OmI",
   "metadata": {
    "id": "dw5-9A4A4OmI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir ../data/\n",
    "if not os.path.isfile(\"../data/sen1floods11_v1.1.tar.gz\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1lRw3X7oFNq_WyzBO6uyUJijyTuYm23VS\", '../data/sen1floods11_v1.1.tar.gz')\n",
    "\n",
    "!tar -xzf ../data/sen1floods11_v1.1.tar.gz -C ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f776454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "dataset_path = Path(\"../data/sen1floods11_v1.1\")\n",
    "BUCKET_NAME = <BUCKET_NAME>\n",
    "\n",
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "all_files = sagemaker_session.upload_data(path=f'{dataset_path}', bucket=BUCKET_NAME, key_prefix='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b7e3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed_everything': 42,\n",
       " 'trainer': {'accelerator': 'auto',\n",
       "  'strategy': 'auto',\n",
       "  'devices': 'auto',\n",
       "  'num_nodes': 1,\n",
       "  'precision': '16-mixed',\n",
       "  'logger': True,\n",
       "  'callbacks': [{'class_path': 'RichProgressBar'},\n",
       "   {'class_path': 'LearningRateMonitor',\n",
       "    'init_args': {'logging_interval': 'epoch'}},\n",
       "   {'class_path': 'ModelCheckpoint',\n",
       "    'init_args': {'dirpath': '../output/burnscars/checkpoints',\n",
       "     'mode': 'max',\n",
       "     'monitor': 'val/Multiclass_Jaccard_Index',\n",
       "     'filename': 'best-mIoU',\n",
       "     'save_weights_only': True}}],\n",
       "  'max_epochs': 100,\n",
       "  'log_every_n_steps': 5,\n",
       "  'default_root_dir': '../output/terramind_base_sen1floods11/'},\n",
       " 'data': {'class_path': 'terratorch.datamodules.GenericMultiModalDataModule',\n",
       "  'init_args': {'task': 'segmentation',\n",
       "   'batch_size': 4,\n",
       "   'num_workers': 4,\n",
       "   'modalities': ['S2L1C', 'S1GRD'],\n",
       "   'rgb_modality': 'S2L1C',\n",
       "   'rgb_indices': [3, 2, 1],\n",
       "   'train_data_root': {'S2L1C': '../data/sen1floods11_v1.1/data/S2L1CHand',\n",
       "    'S1GRD': '../data/sen1floods11_v1.1/data/S1GRDHand'},\n",
       "   'train_label_data_root': '../data/sen1floods11_v1.1/data/LabelHand',\n",
       "   'val_data_root': {'S2L1C': '../data/sen1floods11_v1.1/data/S2L1CHand',\n",
       "    'S1GRD': '../data/sen1floods11_v1.1/data/S1GRDHand'},\n",
       "   'val_label_data_root': '../data/sen1floods11_v1.1/data/LabelHand',\n",
       "   'test_data_root': {'S2L1C': '../data/sen1floods11_v1.1/data/S2L1CHand',\n",
       "    'S1GRD': '../data/sen1floods11_v1.1/data/S1GRDHand'},\n",
       "   'test_label_data_root': '../data/sen1floods11_v1.1/data/LabelHand',\n",
       "   'train_split': '../data/sen1floods11_v1.1/splits/flood_train_data.txt',\n",
       "   'val_split': '../data/sen1floods11_v1.1/splits/flood_valid_data.txt',\n",
       "   'test_split': '../data/sen1floods11_v1.1/splits/flood_test_data.txt',\n",
       "   'image_grep': {'S2L1C': '*_S2Hand.tif', 'S1GRD': '*_S1Hand.tif'},\n",
       "   'label_grep': '*_LabelHand.tif',\n",
       "   'no_label_replace': -1,\n",
       "   'no_data_replace': 0,\n",
       "   'num_classes': 2,\n",
       "   'means': {'S2L1C': [2357.089,\n",
       "     2137.385,\n",
       "     2018.788,\n",
       "     2082.986,\n",
       "     2295.651,\n",
       "     2854.537,\n",
       "     3122.849,\n",
       "     3040.56,\n",
       "     3306.481,\n",
       "     1473.847,\n",
       "     506.07,\n",
       "     2472.825,\n",
       "     1838.929],\n",
       "    'S2L2A': [1390.458,\n",
       "     1503.317,\n",
       "     1718.197,\n",
       "     1853.91,\n",
       "     2199.1,\n",
       "     2779.975,\n",
       "     2987.011,\n",
       "     3083.234,\n",
       "     3132.22,\n",
       "     3162.988,\n",
       "     2424.884,\n",
       "     1857.648],\n",
       "    'S1GRD': [-12.599, -20.293],\n",
       "    'S1RTC': [-10.93, -17.329],\n",
       "    'RGB': [87.271, 80.931, 66.667],\n",
       "    'DEM': [670.665]},\n",
       "   'stds': {'S2L1C': [1624.683,\n",
       "     1675.806,\n",
       "     1557.708,\n",
       "     1833.702,\n",
       "     1823.738,\n",
       "     1733.977,\n",
       "     1732.131,\n",
       "     1679.732,\n",
       "     1727.26,\n",
       "     1024.687,\n",
       "     442.165,\n",
       "     1331.411,\n",
       "     1160.419],\n",
       "    'S2L2A': [2106.761,\n",
       "     2141.107,\n",
       "     2038.973,\n",
       "     2134.138,\n",
       "     2085.321,\n",
       "     1889.926,\n",
       "     1820.257,\n",
       "     1871.918,\n",
       "     1753.829,\n",
       "     1797.379,\n",
       "     1434.261,\n",
       "     1334.311],\n",
       "    'S1GRD': [5.195, 5.89],\n",
       "    'S1RTC': [4.391, 4.459],\n",
       "    'RGB': [58.767, 47.663, 42.631],\n",
       "    'DEM': [951.272]},\n",
       "   'train_transform': [{'class_path': 'albumentations.D4'},\n",
       "    {'class_path': 'ToTensorV2'}]}},\n",
       " 'model': {'class_path': 'terratorch.tasks.SemanticSegmentationTask',\n",
       "  'init_args': {'model_factory': 'EncoderDecoderFactory',\n",
       "   'model_args': {'backbone': 'terramind_v1_base',\n",
       "    'backbone_pretrained': True,\n",
       "    'backbone_modalities': ['S2L1C', 'S1GRD'],\n",
       "    'backbone_merge_method': 'mean',\n",
       "    'necks': [{'name': 'SelectIndices', 'indices': [2, 5, 8, 11]},\n",
       "     {'name': 'ReshapeTokensToImage', 'remove_cls_token': False},\n",
       "     {'name': 'LearnedInterpolateToPyramidal'}],\n",
       "    'decoder': 'UNetDecoder',\n",
       "    'decoder_channels': [512, 256, 128, 64],\n",
       "    'head_dropout': 0.1,\n",
       "    'num_classes': 2},\n",
       "   'loss': 'dice',\n",
       "   'ignore_index': -1,\n",
       "   'freeze_backbone': True,\n",
       "   'freeze_decoder': False,\n",
       "   'class_names': ['Others', 'Flood']}},\n",
       " 'optimizer': {'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 2e-05}},\n",
       " 'lr_scheduler': {'class_path': 'ReduceLROnPlateau',\n",
       "  'init_args': {'monitor': 'val/loss', 'factor': 0.5, 'patience': 5}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check config file.\n",
    "import yaml\n",
    "with open('../configs/terramind_v1_base_sen1floods11.yaml') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0453502fb0bf62",
   "metadata": {},
   "source": [
    "# TerraTorch model registry\n",
    "\n",
    "TerraTorch includes its own backbone registry with many EO FMs. It also includes meta registries for all model components that include other sources like timm image models or SMP decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970183baaea88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.registry import BACKBONE_REGISTRY, TERRATORCH_BACKBONE_REGISTRY, TERRATORCH_DECODER_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d15609",
   "metadata": {},
   "source": [
    "**Note:** TiM models are using the Thinking-in-Modalities approach, see our [paper](https://arxiv.org/abs/2504.11171) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4109f8f262cc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all TerraMind v1 backbones.  \n",
    "[backbone   \n",
    " for backbone in TERRATORCH_BACKBONE_REGISTRY\n",
    " if 'terramind_v1' in backbone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51fdde4e1e5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available decoders. We use the UNetDecoder in this example.\n",
    "list(TERRATORCH_DECODER_REGISTRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/opt/ml/data/'\n",
    "\n",
    "# Parameters to modify\n",
    "identifier = <identifier>\n",
    "\n",
    "# Usecase definition\n",
    "usecase = \"terramind_base_sen1floods11\"\n",
    "\n",
    "#local data path\n",
    "data_path = '../data/sen1floods11_v1.1'\n",
    "\n",
    "# Output base directory\n",
    "output_folder = f\"{root_dir}/output\"\n",
    "\n",
    "# Default root dir for all data and splits\n",
    "default_root_dir = f\"{output_folder}/terramind_base_sen1floods11/\"\n",
    "\n",
    "# Path to save checkpoints in\n",
    "checkpoint_path = f\"{output_folder}/burnscars/checkpoints\"\n",
    "\n",
    "# Define standardization values. We use the pre-training values here and providing the additional modalities is not a problem, \n",
    "# which makes it simple to experiment with different modality combinations. \n",
    "# Alternatively, use the dataset statistics that you can generate using \n",
    "# `terratorch compute_statistics -c config.yaml` (requires concat_bands: true for this multimodal datamodule).\n",
    "data_mean = {\n",
    "    \"S2L1C\": [2357.089, 2137.385, 2018.788, 2082.986, 2295.651, 2854.537, 3122.849, 3040.560, 3306.481, 1473.847, 506.070, 2472.825, 1838.929],\n",
    "    \"S2L2A\": [1390.458, 1503.317, 1718.197, 1853.910, 2199.100, 2779.975, 2987.011, 3083.234, 3132.220, 3162.988, 2424.884, 1857.648],\n",
    "    \"S1GRD\": [-12.599, -20.293],\n",
    "    \"S1RTC\": [-10.93, -17.329],\n",
    "    \"RGB\": [87.271, 80.931, 66.667],\n",
    "    \"DEM\": [670.665]\n",
    "}\n",
    "\n",
    "data_stds = {\n",
    "    \"S2L1C\": [1624.683, 1675.806, 1557.708, 1833.702, 1823.738, 1733.977, 1732.131, 1679.732, 1727.26, 1024.687, 442.165, 1331.411, 1160.419],\n",
    "    \"S2L2A\": [2106.761, 2141.107, 2038.973, 2134.138, 2085.321, 1889.926, 1820.257, 1871.918, 1753.829, 1797.379, 1434.261, 1334.311],\n",
    "    \"S1GRD\": [5.195, 5.890],\n",
    "    \"S1RTC\": [4.391, 4.459],\n",
    "    \"RGB\": [58.767, 47.663, 42.631],\n",
    "    \"DEM\": [951.272],\n",
    "}\n",
    "\n",
    "# Batches of examples to use at a time\n",
    "batch_size = 4\n",
    "\n",
    "# Number of training job workers\n",
    "num_workers = 2\n",
    "\n",
    "# Number of classes for segmentation\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "modalities = [\"S2L1C\", \"S1GRD\"]\n",
    "rgb_modality = \"S2L1C\"  # Used for plotting. Defaults to the first modality if not provided.\n",
    "rgb_indices = [3,2,1]  # RGB channel positions in the rgb_modality.\n",
    "\n",
    "# Define data paths as dicts using the modality names as keys.\n",
    "train_data_root = {\n",
    "    \"S2L1C\": f\"{default_root_dir}/S2L1CHand\",\n",
    "    \"S1GRD\": f\"{default_root_dir}/S1GRDHand\",\n",
    "}\n",
    "train_label_data_root = f\"{default_root_dir}/LabelHand\"\n",
    "val_data_root = {\n",
    "    \"S2L1C\": f\"{default_root_dir}/S2L1CHand\",\n",
    "    \"S1GRD\": f\"{default_root_dir}/S1GRDHand\",\n",
    "}\n",
    "val_label_data_root = f\"{default_root_dir}/LabelHand\"\n",
    "test_data_root = {\n",
    "    \"S2L1C\": f\"{default_root_dir}/S2L1CHand\",\n",
    "    \"S1GRD\": f\"{default_root_dir}/S1GRDHand\",\n",
    "}\n",
    "test_label_data_root = f\"{default_root_dir}/LabelHand\"\n",
    "\n",
    "# Define split files because all samples are saved in the same folder.\n",
    "train_split = f\"{default_root_dir}/splits/flood_train_data.txt\"\n",
    "val_split = f\"{default_root_dir}/splits/flood_valid_data.txt\"\n",
    "test_split = f\"{default_root_dir}/splits/flood_test_data.txt\"\n",
    "\n",
    "# Define suffix, again using dicts.\n",
    "img_grep = {\n",
    "    \"S2L1C\": \"*_S2Hand.tif\",\n",
    "    \"S1GRD\": \"*_S1Hand.tif\",\n",
    "}\n",
    "label_grep = \"*_LabelHand.tif\"\n",
    "\n",
    "# With TerraTorch, you can select a subset of the dataset bands as model inputs by providing dataset_bands (all bands in the data) and output_bands (selected bands). This setting is optional for all modalities and needs to be provided as dicts.\n",
    "# Here is an example for with S-1 GRD. You could change the output to [\"VV\"] to only train on the first band. Note that means and stds must be aligned with the output_bands (equal length of values). \n",
    "dataset_bands = {\n",
    "    \"S1GRD\": [\"VV\", \"VH\"]\n",
    "}\n",
    "output_bands = {\n",
    "    \"S1GRD\": [\"VV\", \"VH\"]\n",
    "}\n",
    "\n",
    "no_label_replace = -1,  # Replace NaN labels. defaults to -1 which is ignored in the loss and metrics.\n",
    "no_data_replace = 0,  # Replace NaN data\n",
    "\n",
    "terramind_backbone = 'terramind_v1_base' # choice of 'terramind_v1_base', 'terramind_v1_base_tim', 'terramind_v1_large', 'terramind_v1_large_tim'\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "indices = [5, 11, 17, 23]\n",
    "if 'terramind_v1_base' in terramind_backbone:\n",
    "    indices = [2, 5, 8, 11]  # indices for terramind_v1_base\n",
    "elif 'terramind_v1_large' in terramind_backbone: \n",
    "    indices = [5, 11, 17, 23]  # indices for terramind_v1_large\n",
    "\n",
    "# Define standardization values. We use the pre-training values here and providing the additional modalities is not a problem, which makes it simple to experiment with different modality combinations. Alternatively, use the dataset statistics that you can generate using `terratorch compute_statistics -c config.yaml` (requires concat_bands: true for this multimodal datamodule).\n",
    "data_mean = {\n",
    "    \"S2L1C\": [2357.089, 2137.385, 2018.788, 2082.986, 2295.651, 2854.537, 3122.849, 3040.560, 3306.481, 1473.847, 506.070, 2472.825, 1838.929],\n",
    "    \"S2L2A\": [1390.458, 1503.317, 1718.197, 1853.910, 2199.100, 2779.975, 2987.011, 3083.234, 3132.220, 3162.988, 2424.884, 1857.648],\n",
    "    \"S1GRD\": [-12.599, -20.293],\n",
    "    \"S1RTC\": [-10.93, -17.329],\n",
    "    \"RGB\": [87.271, 80.931, 66.667],\n",
    "    \"DEM\": [670.665]\n",
    "}\n",
    "\n",
    "data_stds = {\n",
    "    \"S2L1C\": [1624.683, 1675.806, 1557.708, 1833.702, 1823.738, 1733.977, 1732.131, 1679.732, 1727.26, 1024.687, 442.165, 1331.411, 1160.419],\n",
    "    \"S2L2A\": [2106.761, 2141.107, 2038.973, 2134.138, 2085.321, 1889.926, 1820.257, 1871.918, 1753.829, 1797.379, 1434.261, 1334.311],\n",
    "    \"S1GRD\": [5.195, 5.890],\n",
    "    \"S1RTC\": [4.391, 4.459],\n",
    "    \"RGB\": [58.767, 47.663, 42.631],\n",
    "    \"DEM\": [951.272],\n",
    "}\n",
    "\n",
    "model_factory=\"EncoderDecoderFactory\",  # Combines a backbone with necks, the decoder, and a head\n",
    "\n",
    "remove_cls_token = False # TerraMind is trained without CLS token, which neads to be specified.\n",
    "\n",
    "freeze_backbone = True, # Only used to speed up fine-tuning in this demo, we highly recommend fine-tuning the backbone for the best performance. \n",
    "freeze_decoder = False,  # Should be false in most cases as the decoder is randomly initialized.\n",
    "class_names = [\"Others\", \"Water\"]  # optionally define class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer specific configurations\n",
    "config['trainer']['default_root_dir'] = default_root_dir\n",
    "config['trainer']['max_epochs'] = max_epochs\n",
    "config['trainer']['callbacks'][-1]['init_args']['dirpath'] = checkpoint_path\n",
    "\n",
    "# Model specific configurations\n",
    "config['model']['init_args']['model_args']['necks'][0]['indices'] = indices\n",
    "config['model']['init_args']['model_args']['backbone'] = terramind_backbone\n",
    "config['model']['init_args']['model_factory'] = model_factory\n",
    "config['model']['init_args']['class_names'] = class_names\n",
    "config['model']['init_args']['freeze_decoder'] = freeze_decoder\n",
    "config['model']['init_args']['freeze_backbone'] = freeze_backbone\n",
    "\n",
    "for index, neck in enumerate(config['model']['init_args']['necks']):\n",
    "    if neck['name'] == 'ReshapeTokensToImage':\n",
    "        config['model']['init_args']['necks'][index]['remove_cls_token'] = remove_cls_token\n",
    "    elif neck['name'] == 'LearnedInterpolateToPyramidal':\n",
    "        # Some decoders like UNet or UperNet expect hierarchical features. \n",
    "        # Therefore, we need to learn a upsampling for the intermediate embedding layers when using a ViT like TerraMind.\n",
    "        continue\n",
    "\n",
    "\n",
    "# Data specific configurations\n",
    "config['data']['init_args']['train_data_root'] = train_data_root\n",
    "config['data']['init_args']['test_data_root'] = test_data_root\n",
    "config['data']['init_args']['val_data_root'] = val_data_root\n",
    "\n",
    "config['data']['init_args']['train_split'] = train_split\n",
    "config['data']['init_args']['val_split'] = val_split\n",
    "config['data']['init_args']['test_split'] = test_split\n",
    "\n",
    "config['data']['init_args']['modalities'] = modalities\n",
    "\n",
    "config['data']['init_args']['label_grep'] = label_grep\n",
    "config['data']['init_args']['image_grep'] = img_grep\n",
    "\n",
    "config['data']['init_args']['num_classes'] = num_classes\n",
    "config['data']['init_args']['no_data_replace'] = no_data_replace\n",
    "config['data']['init_args']['no_label_replace'] = no_label_replace\n",
    "\n",
    "config['data']['init_args']['means'] = data_mean \n",
    "config['data']['init_args']['stds'] = data_stds\n",
    "\n",
    "config['data']['init_args']['batch_size'] = batch_size\n",
    "config['data']['init_args']['num_workers'] = num_workers\n",
    "\n",
    "config['data']['init_args']['num_classes'] = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename configuration file name to user specific filename\n",
    "import os\n",
    "\n",
    "config_filename = f\"{identifier}-flood_terramind.yaml\"\n",
    "config_filepath = f\"../configs/{config_filename}\"\n",
    "with open(config_filepath, 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)\n",
    "\n",
    "# Upload config files to s3 bucket\n",
    "configs = sagemaker_session.upload_data(path=config_filepath, bucket=BUCKET_NAME, key_prefix='data/configs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## Sen1Floods11 Dataset\n",
    "\n",
    "Lets start with analysing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f22dc984ead544",
   "metadata": {},
   "source": [
    "TerraTorch provides generic data modules that work directly with PyTorch Lightning.\n",
    "\n",
    "Sen1Floods11 is a multimodal dataset that provides Sentinel-2 L2A and Sentinel-1 GRD data. \n",
    "Therefore, we are using the `GenericMultiModalDataModule`. \n",
    "This module is similar to the `GenericNonGeoSegmentationDataModule`, which is used for standard segmentation tasks.\n",
    "However, the data roots, `img_grep` are other settings are provided as dict to account for the multimodal inputs. You find all settings in the [documentation](https://ibm.github.io/terratorch/stable/generic_datamodules/). \n",
    "In a Lightning config, the data module is defined with the `data` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f9c2593c90b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting both modalities\n",
    "def plot_sample(sample):\n",
    "    s1 = sample['image']['S1GRD'].cpu().numpy()\n",
    "    s2 = sample['image']['S2L1C'].cpu().numpy()\n",
    "    mask = sample['mask'].cpu().numpy()\n",
    "    \n",
    "    # Scaling data. Using -30 to 0 scaling for S-1 and 0 - 2000 for S-2. S-1 is visualized as [VH, VV, VH]\n",
    "    s1 = (s1.clip(-30, 0) / 30 + 1) * 255\n",
    "    s2 = (s2.clip(0, 2000) / 2000) * 255\n",
    "    s1_rgb = np.stack([s1[1], s1[0], s1[1]], axis=0).astype(np.uint8).transpose(1,2,0)\n",
    "    s2_rgb = s2[[3,2,1]].astype(np.uint8).transpose(1,2,0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax[0].imshow(s1_rgb)\n",
    "    ax[0].set_title('S-1 GRD')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(s2_rgb)\n",
    "    ax[1].set_title('S-2 GRD')\n",
    "    ax[1].axis('off')   \n",
    "    ax[2].imshow(mask, vmin=-1, vmax=1, interpolation='nearest')\n",
    "    ax[2].set_title('Mask')\n",
    "    ax[2].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb92b1d0599a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some samples\n",
    "plot_sample()\n",
    "plot_sample()\n",
    "plot_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a30ddef8ed5a",
   "metadata": {},
   "source": [
    "# Fine-tune TerraMind via PyTorch Lightning\n",
    "\n",
    "With TerraTorch, we can use standard Lightning components for the fine-tuning.\n",
    "These include callbacks and the trainer class.\n",
    "TerraTorch provides EO-specific tasks that define the training and validation steps.\n",
    "In this case, we are using the `SemanticSegmentationTask`.\n",
    "We refer to the [TerraTorch paper](https://arxiv.org/abs/2503.20563) for a detailed explanation of the TerraTorch tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup variables for training using sagemaker\n",
    "\n",
    "name = f'{identifier}-sagemaker'\n",
    "role = get_execution_role()\n",
    "input_s3_uri = f\"s3://{BUCKET_NAME}/data\"\n",
    "model_name = f\"{identifier}-workshop.ckpt\",\n",
    "\n",
    "environment_variables = {\n",
    "    'CONFIG_FILE': f\"{base_path}/configs/{config_filename}\",\n",
    "    'MODEL_DIR': model_path,\n",
    "    'MODEL_NAME': model_name,\n",
    "    'S3_URL': input_s3_uri,\n",
    "    'ROLE_ARN': role,\n",
    "    'ROLE_NAME': role.split('/')[-1],\n",
    "    'EVENT_TYPE': usecase,\n",
    "    'VERSION': 'v1'\n",
    "}\n",
    "\n",
    "ecr_container_url = '637423382292.dkr.ecr.us-west-2.amazonaws.com/eo_training:latest'\n",
    "sagemaker_role = 'SageMaker-ExecutionRole-20240206T151814'\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "instance_count = 1\n",
    "memory_volume = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ac613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Establish an estimator (model) using sagemaker and the configurations from the previous cell.\n",
    "estimator = Estimator(image_uri=ecr_container_url,\n",
    "                      role=get_execution_role(),\n",
    "                      base_job_name=name,\n",
    "                      instance_count=1,\n",
    "                      environment=environment_variables,\n",
    "                      instance_type=instance_type)\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save important values in a file for reuse.\n",
    "export_values = {\n",
    "    'identifier': identifier,\n",
    "    'model_name': model_name,\n",
    "    'config_filename': config_filename,\n",
    "    'bucket_name': BUCKET_NAME\n",
    "}\n",
    "\n",
    "with open('../variables.yaml', 'w') as variable_export:\n",
    "    yaml.dump(export_values, variable_export, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bebdb7370a174",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a77263-5308-4781-a17f-a35e62ca1875",
   "metadata": {
    "id": "35a77263-5308-4781-a17f-a35e62ca1875",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's test the fine-tuned model\n",
    "best_ckpt_path = f\"{output_folder}/terramind_base_sen1floods11/checkpoints/best-mIoU.ckpt\"\n",
    "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)\n",
    "\n",
    "# Note: This demo only trains for 5 epochs by default, which does not result in good test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
    "outputId": "c7c06228-e634-4608-cb0e-617d003fea46"
   },
   "outputs": [],
   "source": [
    "# Now we can use the model for predictions and plotting\n",
    "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "    best_ckpt_path,\n",
    "    model_factory=model.hparams.model_factory,\n",
    "    model_args=model.hparams.model_args,\n",
    ")\n",
    "\n",
    "test_loader = datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "    images = batch[\"image\"]\n",
    "    for mod, value in images.items():\n",
    "        images[mod] = value.to(model.device)\n",
    "    masks = batch[\"mask\"].numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    \n",
    "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(4):\n",
    "    sample = {\n",
    "        \"image\": batch[\"image\"][\"S2L1C\"][i].cpu(),\n",
    "        \"mask\": batch[\"mask\"][i],\n",
    "        \"prediction\": preds[i],\n",
    "    }\n",
    "    test_dataset.plot(sample)\n",
    "    plt.show()\n",
    "    \n",
    "# Note: This demo only trains for 5 epochs by default, which does not result in good predictions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
