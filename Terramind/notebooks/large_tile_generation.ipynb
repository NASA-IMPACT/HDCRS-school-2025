{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6872e1a26474b6",
   "metadata": {},
   "source": [
    "# Large-tile generation\n",
    "\n",
    "TerraMind was pre-trained on small patches of 224x224 pixels. Passing larger inputs to the model works as long as it is a multiple of 16x16 pixel. However, this is outside the training scope and can lead to worse generation results or OOM errors. \n",
    "This example performs generation of a larger tile using the `tiled_inference` function provided by TerraTorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "import os\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import sagemaker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotting_utils import s2_to_rgb, s1_to_rgb, dem_to_rgb, ndvi_to_rgb, lulc_to_rgb\n",
    "from huggingface_hub import hf_hub_download\n",
    "from plotting_utils import plot_s2, plot_modality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6e86fa59dfbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Santiago large-scale example from Hugging Face (2000x2000 pixel)\n",
    "if not os.path.isfile('../examples/S2L2A/Santiago.tif'):\n",
    "    hf_hub_download(repo_id='ibm-esa-geospatial/Examples', filename='S2L2A/Santiago.tif', repo_type='dataset', local_dir='../examples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30e699361d4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Singpore large-scale example from Hugging Face (2000x2000 pixel)\n",
    "if not os.path.isfile('../examples/S2L2A/Singapore_2025-01-09.tif'):\n",
    "    hf_hub_download(repo_id='ibm-esa-geospatial/Examples', filename='S2L2A/Singapore_2025-01-09.tif', repo_type='dataset', local_dir='../examples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a1497474ca1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Santiago tile (alternative input)\n",
    "data = rxr.open_rasterio('../examples/S2L2A/Santiago.tif').values\n",
    "\n",
    "# Optionally reduce image size to speed up inference\n",
    "data = data[:, 500:1500]\n",
    "\n",
    "# Display the input\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plot_s2(data, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059321429f3bd41",
   "metadata": {},
   "source": [
    "# Run inference\n",
    "\n",
    "`tiled_inference` can be used with any model as long as the output of the passed `forward` function returns a tensor. We therefore wrap the model and stack the generated modalities. \n",
    "\n",
    "```\n",
    "# Define model forward for tiled inference\n",
    "def model_forward(x):\n",
    "    # Run chained generation for all output modalities \n",
    "    generated = model(x)\n",
    "    \n",
    "    # TerraTorch tiled inference expects a tensor output from model forward. We concatenate all generations along channel dimension.     \n",
    "    out = torch.concat([\n",
    "        generated['S1GRD'],\n",
    "        generated['DEM'],\n",
    "        generated['LULC'],\n",
    "        generated['NDVI']\n",
    "    ], dim=1)\n",
    "    \n",
    "    return out \n",
    "\n",
    "pred = tiled_inference(model_forward, input, crop=256, stride=224, batch_size=8, verbose=True)\n",
    "pred = pred.squeeze(0)  # Remove batch dim\n",
    "\n",
    "# Split up the stacked bands into each modality\n",
    "generated = {\n",
    "    'S1GRD': pred[0:2],\n",
    "    'DEM': pred[2],\n",
    "    'LULC': pred[3:13].argmax(0),  # Get class predictions from logits\n",
    "    'NDVI': pred[-1],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d9517",
   "metadata": {},
   "source": [
    "**Note:** However, for our usecase we have the model deployed in a sagemaker endpoint. We will utilize the endpoint to get inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a48ca53500b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "# prepare payload\n",
    "sm = sagemaker.Session().sagemaker_runtime_client\n",
    "\n",
    "## Run this\n",
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "BUCKET_NAME = 's3://hdcrs-school-2025-06031643-tvsscp-001' # replace with your own bucket name once your deployment is done\n",
    "\n",
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "all_files = sagemaker_session.upload_data(path=f'../examples', bucket=BUCKET_NAME, key_prefix='data/examples')\n",
    "endpoint_name = <endpoint name> # replace with your own endpoint name once deployment is done.\n",
    "\n",
    "query = {\n",
    "    'generation': True,\n",
    "    'input_file': f\"s3://{BUCKET_NAME}/data/examples/S2L2A/Santiago.tif\",\n",
    "    'reduce': True\n",
    "}\n",
    "\n",
    "response = sm.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(query),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "\n",
    "generated = json.loads(response['Body'].read())\n",
    "\n",
    "for key in generated:\n",
    "    generated[key] = np.asarray(generated[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20156eb28dd7da2a",
   "metadata": {},
   "source": [
    "# Analyze the generations\n",
    "\n",
    "Let's have a look at the generations. You can notice that all modalities capture the general features quite well but are often wrongly scaled. While S1 and NDVI values are closer to the mean values than the ground trough, DEM generations clearly shows the smaller patches of the tiled inference. The model fails to estimate the general elevation of each single patch. Because of the tiled inference, some generations can look a bit patchy, e.g., S1 below clouds as the model estimates the shoreline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189d05839f52c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generations\n",
    "n_plots = len(generated) + 1\n",
    "fig, ax = plt.subplots(1, n_plots, figsize=(5 * n_plots, 5))\n",
    "\n",
    "plot_s2(data, ax=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "\n",
    "for i, (mod, value) in enumerate(generated.items()):\n",
    "    plot_modality(mod, value, ax=ax[i + 1])\n",
    "\n",
    "    ax[i+1].set_title('generated ' + mod)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd2e2a2f1b787f",
   "metadata": {},
   "source": [
    "# Compare generations in split view\n",
    "\n",
    "We use leafmap for the interactive visualisation of a generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7d7c4131a0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison using leafmap\n",
    "leafmap.image_comparison(\n",
    "    s2_to_rgb(data),\n",
    "    s1_to_rgb(generated['S1GRD']),\n",
    "    # dem_to_rgb(generated['DEM']),\n",
    "    # ndvi_to_rgb(generated['NDVI']),\n",
    "    # lulc_to_rgb(generated['LULC']),\n",
    "    label1=\"S-2 L2A\",\n",
    "    label2=\"Generated data\",\n",
    "    starting_position=50,\n",
    "    out_html=\"image_comparison.html\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
