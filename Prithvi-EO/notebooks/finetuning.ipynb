{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb85522-53a4-43fb-95bc-b37d953662c4",
   "metadata": {},
   "source": [
    "# Foundation Model Fine-tuning Summer School\n",
    "## Using AWS SageMaker for Earth Observation Model Training\n",
    "\n",
    "Welcome to this HDCRS summer school notebook on **practical guide to fine-tuning the Prithvi Earth Observation (EO) v2.0 model**! This hands-on session will teach you how to adapt the pre-trained model for specific Earth observation tasks using cloud-based infrastructure (AWS Sagemker).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "### **Foundation Models & Transfer Learning Concepts**\n",
    "- **Foundation Models Fine-tuning**: The process of taking a pre-trained model and training it further on your specific dataset. (Foundation models are Large pre-trained models trained on vast datasets that capture general patterns and can be adapted for specific tasks)\n",
    "- **Transfer Learning**: Leveraging knowledge learned from one task to improve performance on a related task\n",
    "\n",
    "### **Technical Skills**\n",
    "- How to use **AWS SageMaker** for model training in the cloud\n",
    "- Working with **Hugging Face datasets** for machine learning workflows\n",
    "- Understanding training parameters and their impact on model performance\n",
    "- Deploying the finetuned models for Application consumption\n",
    "\n",
    "### **Real-World Application**\n",
    "- Fine-tuning the **Prithvi Earth Observation (EO) v2.0** model for burn scar detection\n",
    "- Using **Harmonized Landsat Sentinel (HLS)** satellite imagery\n",
    "- Understanding the complete ML pipeline from data preparation to model deployment using cloud resources\n",
    "\n",
    "## Usecase for Hands-on: Burn scar detection using Harmonised Landsat Sentinel and Prithvi EO v2.0\n",
    "\n",
    "**Burn scar detection** is crucial for:\n",
    "- **Environmental monitoring**: Understanding wildfire impact and recovery\n",
    "- **Climate research**: Tracking ecosystem changes over time\n",
    "- **Disaster response**: Rapid assessment of fire damage\n",
    "- **Policy making**: Evidence-based land management decisions\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### **AWS SageMaker**\n",
    "Amazon SageMaker is a fully managed machine learning service that provides:\n",
    "- **Managed training infrastructure**: No need to set up servers or manage hardware\n",
    "- **Scalable compute resources**: From single GPUs to distributed multi-node training\n",
    "- **MLOps capabilities**: Model versioning, monitoring, and deployment\n",
    "\n",
    "### **Prithvi EO Foundation Model**\n",
    "- **Pre-trained on massive satellite imagery datasets**\n",
    "- **Multiple model sizes**: 100M, 300M, and 600M parameters\n",
    "- **Temporal and spatial understanding**: Designed for Earth observation tasks\n",
    "- **Transfer learning ready**: Can be fine-tuned for various geospatial applications\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "**Before starting:**\n",
    "1. Go to \"Kernel\" â†’ Select \"prithvi_eo\" \n",
    "2. Ensure you have AWS credentials configured\n",
    "3. Have an S3 bucket ready for data storage\n",
    "\n",
    "Let's begin our journey into foundation model fine-tuning! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef9538-adf0-4c96-82c8-bab4de94f469",
   "metadata": {},
   "source": [
    "## Libraries Used\n",
    "\n",
    "### **Core Libraries**\n",
    "- **`boto3`**: AWS SDK for Python - allows us to interact with AWS services programmatically\n",
    "- **`sagemaker`**: High-level interface for AWS SageMaker - simplifies model training and deployment\n",
    "- **`rasterio`**: Geospatial raster data processing - reads satellite imagery files\n",
    "\n",
    "### **Hugging Face**\n",
    "- **`huggingface_hub`**: Access to pre-trained models and datasets from Hugging Face Hub\n",
    "- **`snapshot_download`**: Downloads entire repositories (models/datasets) from Hugging Face\n",
    "- **`hf_hub_download`**: Downloads specific files from Hugging Face repositories\n",
    "\n",
    "### **Why These Libraries Matter**\n",
    "Each library serves a specific purpose in our ML pipeline:\n",
    "1. **Data handling**: `rasterio`, `numpy` for processing satellite imagery\n",
    "2. **Cloud infrastructure**: `boto3`, `sagemaker` for scalable training\n",
    "3. **Configuration management**: `yaml` for reproducible experiments\n",
    "4. **Pre-trained assets**: `huggingface_hub` for accessing foundation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae36e6-0ca6-47eb-88a0-9922b0f9db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import yaml\n",
    "import rasterio\n",
    "import sagemaker\n",
    "\n",
    "from datetime import time\n",
    "from glob import glob\n",
    "\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c584a",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "For this hands-on session, Burn Scars example will be used for fine-tuning. All of the data and pre-trained models are available in Huggingface. Huggingface packages and git will be utilized to download, and prepare datasets and pretrained models.\n",
    "\n",
    "->\n",
    "\n",
    "## Understanding Dataset: HLS Burn Scars\n",
    "\n",
    "### **What is HLS Data?**\n",
    "**Harmonized Landsat Sentinel (HLS)** combines data from two major satellite missions:\n",
    "- **Landsat 8/9 (NASA)**: 30m resolution, 16-day revisit cycle\n",
    "- **Sentinel-2 (ESA)**: 10-20m resolution, 5-day revisit cycle\n",
    "\n",
    "**Why combine them?**\n",
    "- **Higher temporal resolution**: More frequent observations for change detection\n",
    "- **Consistent data format**: Harmonized processing makes data easier to use\n",
    "- **Better coverage**: Reduces data gaps from cloud cover or orbit patterns\n",
    "\n",
    "### **Burn Scar Dataset**\n",
    "The dataset we'll use contains:\n",
    "- **Training samples**: HLS imagery with burn scar labels\n",
    "- **Validation samples**: Used to monitor training progress\n",
    "- **Test samples**: Final evaluation of model performance\n",
    "\n",
    "### **Dataset Structure**\n",
    "Each sample typically includes:\n",
    "- **Multi-spectral imagery**: 6 bands (Blue, Green, Red, NIR, SWIR1, SWIR2)\n",
    "- **Labels**: Binary masks (burn scar vs. no burn scar)\n",
    "- **Metadata**: Location, date, acquisition parameters\n",
    "\n",
    "### **Why Hugging Face for Datasets?**\n",
    "- **Standardized format**: Easy to share and reproduce\n",
    "- **Version control**: Track dataset changes over time\n",
    "- **Community access**: Researchers can build upon each other's work\n",
    "- **Integration**: Seamless connection with ML frameworks\n",
    "\n",
    "Let's download and explore our dataset! ðŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a6473-6c86-4882-83d5-73ede824be89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Step 1: Download HLS Burn Scars Dataset\n",
    "\n",
    "**Dataset Source**: [ibm-nasa-geospatial/hls_burn_scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars)\n",
    "\n",
    "**What happens in this step:**\n",
    "1. **`snapshot_download()`**: Downloads the entire dataset repository from Hugging Face\n",
    "2. **`allow_patterns`**: Only downloads the compressed tar.gz file (saves bandwidth)\n",
    "3. **`tar -xvzf`**: Extracts the compressed dataset to our local directory\n",
    "\n",
    "**File Organization:**\n",
    "```\n",
    "../data/hls_burn_scars/\n",
    "â”œâ”€â”€ training/          # Training images and labels\n",
    "â”œâ”€â”€ validation/        # Validation images and labels  \n",
    "â””â”€â”€ test/             # Test images and labels (for final evaluation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c4e10-15cd-4f52-8dfe-ec04074efe43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/hls_burn_scars\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/hls_burn_scars\",\n",
    "    allow_patterns=\"hls_burn_scars.tar.gz\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=DATA_PATH,\n",
    ")\n",
    "!tar -xvzf ../data/hls_burn_scars/hls_burn_scars.tar.gz -C ../data/hls_burn_scars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7f9a5-cb4e-4a51-9c3b-5f8f0ebd7d16",
   "metadata": {},
   "source": [
    "### Step 2: Upload Data to AWS S3\n",
    "\n",
    "**Why use S3?**\n",
    "- **Scalable storage**: Handle datasets of any size\n",
    "- **Global access**: Training jobs can access data from anywhere\n",
    "- **Cost-effective**: Pay only for what you store and transfer\n",
    "- **Integration**: Native integration with SageMaker training jobs\n",
    "\n",
    "**Important Note**: Replace `<BUCKET_NAME>` with your actual S3 bucket name\n",
    "\n",
    "**What's happening here:**\n",
    "1. **`sagemaker.Session()`**: Creates a session to manage AWS interactions\n",
    "2. **`upload_data()`**: Uploads local data to S3 with organized prefixes\n",
    "3. **Key prefixes**: Organize data in S3 like folders (datasets/training/, etc.)\n",
    "\n",
    "**S3 Structure Created:**\n",
    "```\n",
    "s3://your-bucket/\n",
    "â”œâ”€â”€ datasets/\n",
    "â”‚   â”œâ”€â”€ training/      # Training data\n",
    "â”‚   â”œâ”€â”€ validation/    # Validation data\n",
    "â”‚   â””â”€â”€ test/          # Test data\n",
    "â””â”€â”€ data/\n",
    "    â””â”€â”€ configs/       # Configuration files (added later)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f74d4f-8084-4c19-8be3-cb4cebbae855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "\n",
    "BUCKET_NAME = <BUCKET-NAME>\n",
    "\n",
    "# Prepare sagemaker session with files uploaded to s3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "train_images = sagemaker_session.upload_data(path=f'{DATA_PATH}/training', bucket=BUCKET_NAME, key_prefix='data/training')\n",
    "val_images = sagemaker_session.upload_data(path=f'{DATA_PATH}/validation', bucket=BUCKET_NAME, key_prefix='data/validation')\n",
    "test_images = sagemaker_session.upload_data(path=f'{DATA_PATH}/validation', bucket=BUCKET_NAME, key_prefix='data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef75557",
   "metadata": {},
   "source": [
    "## Prepare config and parameters\n",
    "\n",
    "\n",
    "**config file** contains all the settings that determine:\n",
    "- **Model architecture**: Which foundation model to use, how many layers, etc.\n",
    "- **Training parameters**: Learning rate, batch size, number of epochs\n",
    "- **Data settings**: Where to find data, how to preprocess it\n",
    "- **Hardware settings**: GPU usage, memory allocation\n",
    "\n",
    "### **Why Use YAML?**\n",
    "**YAML (Yet Another Markup Language)** is human-readable and perfect for configurations:\n",
    "- **Easy to read**: Clear structure with indentation\n",
    "- **Easy to modify**: Change parameters without touching code\n",
    "- **Version control friendly**: Track configuration changes over time\n",
    "- **Reproducible**: Same config = same experimental conditions\n",
    "\n",
    "### **Key Configuration Sections**\n",
    "1. **Model**: Foundation model selection and architecture\n",
    "2. **Data**: Dataset paths, preprocessing, and augmentation\n",
    "3. **Trainer**: Training parameters, callbacks, and logging\n",
    "4. **Optimization**: Learning rate, optimizer settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377435dd-4f07-4e0d-b4d8-33bed20d0116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../configs/prithvi_v2_eo_300_tl_unet_burnscars.yaml') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad5078-b4e7-4036-8bfb-f46d1f756293",
   "metadata": {},
   "source": [
    "### Dataset Statistics\n",
    "\n",
    "**Why calculate band statistics?**\n",
    "In satellite imagery, different spectral bands have vastly different value ranges. **Normalization** ensures:\n",
    "- **Stable training**: Prevents any single band from dominating the learning process\n",
    "- **Faster convergence**: Helps the model learn more efficiently\n",
    "- **Better performance**: Standardized inputs lead to better feature learning\n",
    "\n",
    "**The normalization Applied:**\n",
    "```\n",
    "normalized_value = (pixel_value - mean) / standard_deviation\n",
    "```\n",
    "\n",
    "This transforms all pixel values to have **mean = 0** and **standard deviation = 1** (called **z-score normalization**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c8ecf-cc92-44aa-b32d-fca260a87688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_band_statistics(image_directory, image_pattern, bands=[0, 1, 2, 3, 4, 5]):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of each band in a folder of GeoTIFF files.\n",
    "\n",
    "    Args:\n",
    "        image_directory (str): Directory where the source GeoTIFF files are stored that are passed to model for training.\n",
    "        image_pattern (str): Pattern of the GeoTIFF file names that globs files for computing stats.\n",
    "        bands (list, optional): List of bands to calculate statistics for. Defaults to [0, 1, 2, 3, 4, 5].\n",
    "\n",
    "    Raises:\n",
    "        Exception: If no images are found in the given directory.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists containing the means and standard deviations of each band.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store the means and standard deviations\n",
    "    all_means = []\n",
    "    all_stds = []\n",
    "\n",
    "    # Use glob to get a list of all .tif images in the directory\n",
    "    all_images = glob(f\"{image_directory}/{image_pattern}\")\n",
    "\n",
    "    # Make sure there are images to process\n",
    "    if not all_images:\n",
    "        raise Exception(\"No images found\")\n",
    "\n",
    "    # Get the number of bands\n",
    "    num_bands = len(bands)\n",
    "\n",
    "    # Initialize arrays to hold sums and sum of squares for each band\n",
    "    band_sums = np.zeros(num_bands)\n",
    "    band_sq_sums = np.zeros(num_bands)\n",
    "    pixel_counts = np.zeros(num_bands)\n",
    "\n",
    "    # Iterate over each image\n",
    "    for image_file in all_images:\n",
    "        with rasterio.open(image_file) as src:\n",
    "            # For each band, calculate the sum, square sum, and pixel count\n",
    "            for band in bands:\n",
    "                data = src.read(band + 1)  # rasterio band index starts from 1\n",
    "                band_sums[band] += np.nansum(data)\n",
    "                band_sq_sums[band] += np.nansum(data**2)\n",
    "                pixel_counts[band] += np.count_nonzero(~np.isnan(data))\n",
    "\n",
    "    # Calculate means and standard deviations for each band\n",
    "    for i in bands:\n",
    "        mean = band_sums[i] / pixel_counts[i]\n",
    "        std = np.sqrt((band_sq_sums[i] / pixel_counts[i]) - (mean**2))\n",
    "        all_means.append(mean)\n",
    "        all_stds.append(std)\n",
    "\n",
    "    return all_means, all_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a856a9",
   "metadata": {},
   "source": [
    "# Configuration for training\n",
    "\n",
    "-   `identifier`: This variable will be used as a prefix for all artifacts related to fine-tuning and deployments. Please update it with an appropriate identifier.\n",
    "-   `usecase`: This variable refers to the use cases the Prithvi model will be fine-tuned for, e.g., `burn_scars`, `flood_detection`, etc. For this hands-on session, we will be using `burn_scars`. If you have your own data, please update accordingly.\n",
    "-   `data_path`: Data path is where the data locally resides. This will be used to find the files for fine-tuning. These files will then be used to calculate statistics like `mean` and `standard deviation`. These files will also be uploaded to an S3 bucket for the training job to use.\n",
    "-   `batch_size`: This is the number of data samples processed by the model in one iteration during training. We are using `4` by default. Depending on the availability of GPUs and resources, this can be increased.\n",
    "-   `num_workers`: This is the number of worker processes used for data loading during training. We are using `2` by default. This can be adjusted based on CPU and I/O capabilities.\n",
    "-   `num_classes`: This variable represents the number of classes in the fine-tuning job. For `burn_scars`, we have two classes: `burn_scar` and `no_burn_scar`. Update it according to the data you are using for training.\n",
    "-   `prithvi_backbone`: This variable represents the Prithvi Earth Observation Foundation Model (EO FM) pre-trained using HLS data. There are several variations:\n",
    "    -   `prithvi_eo_v1_100`: This is an older version of the Prithvi EO FM. It will not be used in this hands-on session.\n",
    "    -   `prithvi_eo_v2_300`: This version of the Prithvi EO FM has approximately 300 million parameters (typically around 24 Transformer encoder layers). It can be selected for faster training and a lower memory footprint.\n",
    "    -   `prithvi_eo_v2_300_tl`: This version also has ~300 million parameters (typically around 24 Transformer encoder layers) and is pre-trained with **T**emporal and **L**ocation embeddings. It is ideal for fine-tuning use cases where spatial and temporal information is important and a smaller footprint is desired. For example, crop classification using imagery from multiple time steps.\n",
    "    -   `prithvi_eo_v2_600`: This is a larger version of the Prithvi EO FM with approximately 600 million parameters (typically 32 Transformer encoder layers). It can be selected for use cases requiring high precision, accuracy, or recall. Note: the memory footprint of this model is significantly larger than the 300M versions. Ensure sufficient resources are available.\n",
    "    -   `prithvi_eo_v2_600_tl`: This version also has ~600 million parameters (typically 32 Transformer encoder layers) and includes **T**emporal and **L**ocation embeddings. It is best suited for high-performance fine-tuning on use cases where precise spatial and temporal information is crucial, such as detailed change detection or multi-temporal crop type mapping. The resource considerations are similar to the `prithvi_eo_v2_600` model.\n",
    "-   `base_path`: This variable specifies the base directory for training operations, including the path for input data, configuration files, and the location for storing model artifacts post-training. For SageMaker training jobs, `/opt/ml/data` is commonly used. If using a different environment, please update accordingly.\n",
    "-   `max_epochs`: This variable limits the number of `epochs` (full passes through the training dataset) a fine-tuning job runs for. A higher number of epochs equates to longer training time and may lead to better-performing models, but this needs to be validated on a case-by-case basis to avoid overfitting.\n",
    "-   `indices`: For most of our fine-tuning jobs, a `decoder` is added on top of the selected Prithvi backbone. This variable specifies which Transformer blocks (by their index) from the Prithvi backbone will provide their output feature embeddings to this decoder. Commonly, features from blocks at or around 1/4, 1/2, 3/4, and the final depth of the backbone are used to capture multi-scale information. The selection of these indices impacts the architecture and parameter count of the decoder, not the Prithvi backbone itself.\n",
    "-   `means`: This is the mean of the pixel values across each channel of the training dataset. The mean values, along with standard deviations, will be used for zero-center normalization of input values.\n",
    "-   `stds`: This is the standard deviation of the pixel values across each channel of the training dataset. The standard deviation values, along with means, will be used for zero-center normalization of input values.\n",
    "-   `model_path`: This variable specifies where the model artifacts will be stored after training.\n",
    "\n",
    "->\n",
    "\n",
    "## Training Configuration Parameters\n",
    "\n",
    "Each parameter plays a crucial role in determining your model's training behavior and performance. Let's understand what each one does:\n",
    "\n",
    "### ** Experiment Management**\n",
    "- **`identifier`**: Your unique experiment name (like \"summer_school_2025\")\n",
    "  - *Why important*: Helps organize and track different training runs\n",
    "  - *Best practice*: Use descriptive names with dates or versions\n",
    "\n",
    "- **`usecase`**: The specific task you're solving (\"burn_scars\", \"flood_detection\", etc.)\n",
    "  - *Why important*: Organizes model artifacts by application\n",
    "  - *Example*: \"burn_scars\" for wildfire damage assessment\n",
    "\n",
    "### **Data Configuration**\n",
    "- **`data_path`**: Local path to your training data\n",
    "  - *Purpose*: Points to images for statistics calculation and initial processing\n",
    "  - *Note*: Data gets uploaded to S3 for actual training\n",
    "\n",
    "- **`batch_size`**: Number of images processed together (default: 4)\n",
    "  - *Trade-offs*: Larger = faster training but more memory usage\n",
    "  - *GPU memory consideration*: Reduce if you get \"out of memory\" errors\n",
    "\n",
    "- **`num_workers`**: Parallel processes for data loading (default: 2)\n",
    "  - *Purpose*: Speeds up data loading while model trains\n",
    "  - *Optimization*: Usually set to number of CPU cores available\n",
    "\n",
    "- **`num_classes`**: Number of prediction categories (2 for burn_scar vs. no_burn_scar)\n",
    "  - *Critical*: Must match your dataset's label structure\n",
    "\n",
    "### **Foundation Model Selection**\n",
    "\n",
    "The **`prithvi_backbone`** determines which pre-trained model you start with:\n",
    "\n",
    "| Model | Parameters | Layers | Use Case | Memory |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| `prithvi_eo_v2_300` | 300M | 24 | General purpose, faster training | Lower |\n",
    "| `prithvi_eo_v2_300_tl` | 300M | 24 | **T**emporal + **L**ocation aware | Lower |\n",
    "| `prithvi_eo_v2_600` | 600M | 32 | High precision tasks | Higher |\n",
    "| `prithvi_eo_v2_600_tl` | 600M | 32 | Temporal analysis, change detection | Higher |\n",
    "\n",
    "**Choosing the right model:**\n",
    "- **300M models**: Great for learning and most applications\n",
    "- **600M models**: When you need maximum accuracy\n",
    "- **TL versions**: When time/location context matters\n",
    "\n",
    "### **Training Parameters**\n",
    "- **`max_epochs`**: How many times to see the entire dataset\n",
    "  - *Rule of thumb*: Start with 10-50, monitor validation performance\n",
    "  - *Warning*: Too many epochs can cause overfitting\n",
    "\n",
    "- **`indices`**: Which model layers to use for the decoder\n",
    "  - *Purpose*: Extracts features at different scales (like zoom levels)\n",
    "  - *Multi-scale learning*: Combines coarse and fine-grained features\n",
    "\n",
    "### **Normalization Statistics**\n",
    "- **`means`** & **`stds`**: Dataset-specific normalization values\n",
    "  - *Why critical*: Ensures your data matches what the foundation model expects\n",
    "  - *Calculated automatically*: From your training data\n",
    "\n",
    "### **Set Your Parameters**\n",
    "\n",
    "**Replace the placeholder values with your specific settings:**\n",
    "\n",
    "```python\n",
    "# REQUIRED: Replace these with your values\n",
    "identifier = \"your_name_summer_school_2025\"  # Your unique identifier\n",
    "usecase = \"burn_scars\"                       # Keep as burn_scars for this exercise\n",
    "```\n",
    "\n",
    "**ðŸ’¡ Parameter Recommendations for Learning:**\n",
    "- Keep `batch_size = 4` (good for learning, won't overwhelm GPU memory)\n",
    "- Start with `max_epochs = 10` (faster for experimentation)\n",
    "- Use `prithvi_eo_v2_300` (smaller model, faster training)\n",
    "\n",
    "**ðŸš€ Advanced Students:**\n",
    "- Try `prithvi_eo_v2_300_tl` for temporal understanding\n",
    "- Increase `max_epochs` to 50-100 for better performance\n",
    "- Experiment with `batch_size = 8` if you have sufficient GPU memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe29f1-83f6-4b11-a0e9-67c83f4e6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = \"mr01eo\"\n",
    "usecase = \"burnscars\"\n",
    "#local data path\n",
    "data_path = '../data/hls_burn_scars/'\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8494e52-c71b-4626-a579-70fecb7e584c",
   "metadata": {},
   "source": [
    "**What's happening in this cell:**\n",
    "\n",
    "1. **Dynamic Model Architecture Setup**: The `indices` are automatically set based on your chosen backbone\n",
    "   - **Why different indices?**: Each model has different numbers of layers\n",
    "   - **Multi-scale features**: We extract features from 1/4, 1/2, 3/4, and final model depths\n",
    "\n",
    "2. **Dataset Statistics Calculation**: \n",
    "   - **`calculate_band_statistics()`**: Computes mean and std for each spectral band\n",
    "   - **Pattern matching**: `'training/*_merged.tif'` finds all training images\n",
    "   - **Critical for normalization**: Ensures model sees appropriately scaled data\n",
    "\n",
    "3. **Setting normalization parameters**: calculated means and stds from the training data\n",
    "   - **Configuring training duration**: Number of epochs for the training process\n",
    "   - **Setting up data paths**: Tells SageMaker where to find training, validation, and test data\n",
    "   - **Organizing outputs**: Specifies where to save training logs and model checkpoints\n",
    "\n",
    "\n",
    "**IMPORTANT**: Notice how we configure different indices for different model sizes - this is a key aspect of working with foundation models of varying depths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086402bf-a03e-46e1-b62a-275e097b7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model backbone can be either:\n",
    "  - prithvi_eo_v1_100\n",
    "  - prithvi_eo_v2_300\n",
    "  - prithvi_eo_v2_300_tl\n",
    "  - prithvi_eo_v2_600\n",
    "  - prithvi_eo_v2_600_tl\n",
    "\"\"\"\n",
    "prithvi_backbone = 'prithvi_eo_v2_300'\n",
    "\n",
    "base_path = '/opt/ml/data'\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "config['data']['init_args']['batch_size'] = batch_size\n",
    "config['data']['init_args']['num_workers'] = num_workers\n",
    "\n",
    "config['data']['init_args']['num_classes'] = num_classes\n",
    "\n",
    "\n",
    "config['model']['init_args']['model_args']['backbone'] = prithvi_backbone\n",
    "\n",
    "\n",
    "indices = [5, 11, 17, 23]\n",
    "if 'prithvi_eo_v2_100' in prithvi_backbone:\n",
    "    indices = [2, 5, 8, 11]  # indices for prithvi_eo_v1_100\n",
    "elif 'prithvi_eo_v2_300' in prithvi_backbone:\n",
    "    indices = [5, 11, 17, 23]  # indices for prithvi_eo_v2_300\n",
    "elif 'prithvi_eo_v2_600' in prithvi_backbone:\n",
    "    indices = [7, 15, 23, 31]  # indices for prithvi_eo_v2_600\n",
    "\n",
    "config['model']['init_args']['model_args']['necks'][0]['indices'] = indices\n",
    "\n",
    "means, stds = calculate_band_statistics(data_path, 'training/*_merged.tif')\n",
    "\n",
    "model_path = f\"{base_path}/{usecase}/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b163ac-8f44-4ab2-8341-c9a9d2ac5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation calculated from the training dataset for all 6 bands,\n",
    "# for zero center normalization.\n",
    "config['data']['init_args']['means'] = [float(val) for val in means]\n",
    "config['data']['init_args']['stds'] = [float(val) for val in stds]\n",
    "\n",
    "# Total number of epochs the training will run for. Since we are short on time,\n",
    "# we will just be running it for 1 epoch. This can be updated to any positive integer.\n",
    "config['trainer']['max_epochs'] = max_epochs\n",
    "\n",
    "config['data']['init_args']['test_data_root'] = f\"{base_path}/test\"\n",
    "config['data']['init_args']['val_data_root'] = f\"{base_path}/validation\"\n",
    "config['data']['init_args']['train_data_root'] = f\"{base_path}/training\"\n",
    "\n",
    "config['trainer']['default_root_dir'] = f\"{base_path}/{usecase}\"\n",
    "\n",
    "config['trainer']['callbacks'][2]['init_args'][\"dirpath\"] = model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1f6a1-43c1-4de5-a34e-8cb9e8295c58",
   "metadata": {},
   "source": [
    "## AWS SageMaker Training Setup\n",
    "\n",
    "### **Understanding SageMaker Training Jobs**\n",
    "\n",
    "**What is a SageMaker Training Job?**\n",
    "- **Managed infrastructure**: AWS provisions and manages compute resources automatically\n",
    "- **Scalable**: Can use multi-GPU clusters\n",
    "- **Pay-per-use**: Only pay for compute time actually used.\n",
    "- **Integrated**: Works with S3, CloudWatch, and other AWS services\n",
    "\n",
    "### **Key Components Explained:**\n",
    "\n",
    "**Environment Variables Recap**\n",
    "- **`CONFIG_FILE`**: Path to your YAML configuration\n",
    "- **`MODEL_DIR`**: Where to save the trained model\n",
    "- **`S3_URL`**: Where your data is stored\n",
    "- **`ROLE_ARN`**: AWS permissions for accessing resources\n",
    "\n",
    "**Instance Types**\n",
    "- **`ml.p3.2xlarge`**: GPU instance good choice for single GPU deep learning\n",
    "  - 1 NVIDIA V100 GPU (16GB memory)\n",
    "  - 8 vCPUs, 61 GB RAM\n",
    "  - Good balance of performance and cost\n",
    "\n",
    "**Container Image**\n",
    "- **ECR (Elastic Container Registry)**: Stores your training environment\n",
    "- **Pre-built image**: Contains all installed necessary libraries and frameworks, Same setup every time you (or your teammate) train\n",
    "\n",
    "**Cost Considerations:**\n",
    "- **On-demand pricing**: 3-4 usd per hour for ml.p3.2xlarge\n",
    "- **Training time**: Usually 1-3 hours for this exercise\n",
    "- **Total cost**: Typically 5-15 usd for a complete training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5139e3c-158a-455f-9369-7110359ccb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename configuration file name to user specific filename\n",
    "import os\n",
    "\n",
    "config_filename = f\"{identifier}-burn_scars_Prithvi-EO.yaml\"\n",
    "config_filepath = f\"../configs/{config_filename}\"\n",
    "with open(config_filepath, 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)\n",
    "\n",
    "# Upload config files to s3 bucket\n",
    "configs = sagemaker_session.upload_data(path=config_filepath, bucket=BUCKET_NAME, key_prefix='data/configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d6b5d-d58d-4b90-a952-6179c255280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables for training using sagemaker\n",
    "\n",
    "name = f'{identifier}-sagemaker'\n",
    "role = get_execution_role()\n",
    "input_s3_uri = f\"s3://{BUCKET_NAME}/data\"\n",
    "model_name = f\"{identifier}-workshop.ckpt\"\n",
    "\n",
    "environment_variables = {\n",
    "    'CONFIG_FILE': f\"{base_path}/configs/{config_filename}\",\n",
    "    'MODEL_DIR': model_path,\n",
    "    'MODEL_NAME': model_name,\n",
    "    'S3_URL': input_s3_uri,\n",
    "    'ROLE_ARN': role,\n",
    "    'ROLE_NAME': role.split('/')[-1],\n",
    "    'EVENT_TYPE': usecase,\n",
    "    'BUCKET_NAME': BUCKET_NAME,\n",
    "    'VERSION': 'v1'\n",
    "}\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_container_url = f'{account_id}.dkr.ecr.us-west-2.amazonaws.com/eo_training:latest'\n",
    "sagemaker_role = role.split('/')[-1]\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "instance_count = 1\n",
    "memory_volume = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b5c5a-9510-4d3f-8b11-dab8fdd8ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish an estimator (model) using sagemaker and the configurations from the previous cell.\n",
    "estimator = Estimator(image_uri=ecr_container_url,\n",
    "                      role=get_execution_role(),\n",
    "                      base_job_name=name,\n",
    "                      instance_count=1,\n",
    "                      environment=environment_variables,\n",
    "                      instance_type=instance_type)\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbb9e6-1b3e-4596-91f9-e3be9f734e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save important values in a file for reuse.\n",
    "export_values = {\n",
    "    'identifier': identifier,\n",
    "    'model_name': model_name,\n",
    "    'config_filename': config_filename,\n",
    "    'bucket_name': BUCKET_NAME\n",
    "}\n",
    "\n",
    "with open('../variables.yaml', 'w') as variable_export:\n",
    "    yaml.dump(export_values, variable_export, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74b9d7-8507-46ba-b65f-234ecd05bb09",
   "metadata": {},
   "source": [
    "## NEXT Lesson: **Model Deployment**: Deploy the trained model for inference\n",
    "continue deploy_and_use.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc09f1c-01a1-4d81-bdd3-e09af2624ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prithvi_eo",
   "language": "python",
   "name": "prithvi_eo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
